{"contributors":[{"path":"https://dat.alets.ch/","role":"author","title":"Oleg"}],"created":"2025-09-20T22:37","description":"The Swiss {ai} Weeks Bern event hosted at the climate-friendly, modern & accessible setting of #SiliconLovefield","homepage":"","keywords":["SwissAIweeks","SiliconLovefield"],"licenses":[{"name":"ODC-PDDL-1.0","path":"http://opendatacommons.org/licenses/pddl/","title":"Open Data Commons Public Domain Dedication & License 1.0"}],"name":"event-1","resources":[{"data":[{"aftersubmit":"The project is to be rated on each of the following evaluation criteria (select 1 from each category below):\r\n\r\n## Technical Functionality\t\r\n\r\n- 5 (exceptional)\timpressive degree of functionality, demonstrates several steps of technical achievement\r\n- 4 (excellent)\tproduct functions, demonstrates 1-2 steps of tech achievement\r\n- 3 (good)\tsome features function, others struggle to convert from idea to reality\r\n- 2 (fair)\tsome features have been attempted, but limited conversion\r\n- 1 (poor)\tremains an idea only and was not implemented at all\r\n\r\n## User Experience\t\r\n\r\n- 5 (exceptional)\tbeautiful, simple to use, responsive design\r\n- 4 (excellent)\tlooks good, but not perfect yet\r\n- 3 (good)\tlooks OK, but laggy and awkward\r\n- 2 (fair)\tsome aspects are realized/implemented, others remain an idea/mockup\r\n- 1 (poor)\tremains an idea only or did not implement a proper user interface\r\n\r\n## Skillful use of AI\t\r\n\r\n- 5 (exceptional)\tskillfull applications of AI throughout the project\r\n- 4 (excellent)\tused AI skillfully on most aspects\r\n- 3 (good)\tuses AI in some aspect, roots on deterministic paradigms\r\n- 2 (fair)\tachieved results without specific use of AI\r\n- 1 (poor)\tmissed the mark completely\r\n\r\n## Uniqueness / Creativity / Fun Factor\t\r\n\r\n- 5 (exceptional)\trock stars - have exceeded expectations and created something very unique that could go viral\r\n- 4 (excellent)\tvery creative \u2013 have not seen something like this before, but most likely will not go viral\r\n- 3 (good)\tseveral creative, fun and original ideas were shown\r\n- 2 (fair)\tidea exists in a similar form, i.e. have seen this before somewhere\r\n- 1 (poor)\tthis idea exists elsewhere in better form\r\n\r\n## Potential / Market Impact\t\r\n\r\n- 5 (exceptional)\tunique, with very high market opportunity\r\n- 4 (excellent)\thigh market opportunity\r\n- 3 (good)\tshowing potential and existing market opportunity\r\n- 2 (fair)\tlow product-market-fit potential\r\n- 1 (poor)\tmarket potential is not understood","boilerplate":"### &#128640; Start a new team!\r\n\r\nFeel free to have a look at the [existing proposals](/event/1), and click \"Join the team\". \r\n\r\n- You can also use the \"Fork\" button to start a new team. \r\n- Use this form only if you are starting a new challenge.\r\n\r\nNeed more help? Get in touch with the [organising team](/event/1/instruction#contacts).\r\n","certificate_path":"","community_embed":"<div class=\"codeofconduct\">Hackathons full of ideas, collaboration, and innovation are based on the premise of keeping the experience safe, inclusive, and respectful for everyone. We follow a clear <b>Code of Conduct</b> and support the <a href=\"https://www.un.org/en/about-us/universal-declaration-of-human-rights\" target=\"_blank\">Universal Declaration of Human Rights</a>. Harassment or discrimination of any kind won't be tolerated\u2014this applies to all staff, participants, coaches, visitors and sponsors. <b>Please take a moment to <a href=\"https://swiss-ai-weeks.ch/code-of-conduct\" target=\"_blank\">review the full guidelines.</a></b></div>\r\n\r\n<p class=\"creativecommons\">The contents of this website, unless otherwise stated, are licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\" target=\"_blank\">Creative Commons Attribution 4.0 International License</a>. The application that powers this site is available under the <a rel=\"sourcecode\" href=\"https://codeberg.org/dribdat/dribdat/src/branch/main/LICENSE\" target=\"_blank\">MIT license</a>.</p>","community_url":"","custom_css":"body.dashboard-page {\r\n  background-size: 60% !important;\r\n  background-position: 50% bottom !important;\r\n}\r\nbody {\r\n  border-left: 10px solid #ff0000;  \r\n  border-right: 10px solid #ff0000;\r\n}\r\n.container.event-header {\r\n  background: #fff; \r\n  border-radius: 8px;\r\n  padding-bottom: 8px;\r\n}\r\n.theme-dark .card-body,\r\n.project-page .project-longtext .card .card-body,\r\n.event-header .text-body, .event-header span, .event-header .event-date,\r\n.bg-light.text-dark a, .project-page.jumbotron a, \r\n.project-page.jumbotron .btn,\r\n.event-home .event-info .event-description.bg-body a, \r\n.event-header .section-header-content a {\r\n  color: #000 !important;\r\n}\r\n\r\n.theme-dark.event-home .event-info .event-description.bg-body {\r\n  background: none !important;\r\n  color: white;\r\n}\r\n\r\ndiv.project-edit-buttons { top: 5px; }\r\n.event-home .event-info .event-description.bg-body { background: white !important; color: black; }\r\n.event-instructions p img { width: 100%; }","description":"<img src=\"https://s3.dribdat.cc/swissai/2025/1/5HAXM/KP54TSBQ.jpg\" width=\"100%\">\r\n\r\n# Presentations\r\n\r\n- \u23ef\ufe0f [Opening & Challenges](https://bbb.ch-open.ch/playback/presentation/2.3/22f4606601cf521f501ba7176dcda6983c0c7838-1758180217576) - [SLIDES](https://s3.dribdat.cc/swissai/2025/1/ECBQW/output.pdf)\r\n- \u23ef\ufe0f [Expert Workshop](https://bbb.ch-open.ch/playback/presentation/2.3/22f4606601cf521f501ba7176dcda6983c0c7838-1758193424408) - [SLIDES](https://s3.dribdat.cc/swissai/2025/1/ZA1QH/S6NVCWMQ.pdf)\r\n- \u23ef\ufe0f [Pitch Workshop](https://bbb.ch-open.ch/playback/presentation/2.3/22f4606601cf521f501ba7176dcda6983c0c7838-1758268934393) - [SLIDES](https://gamma.app/docs/ai-Hackathon-Check-in-2zodw135kf5symu)\r\n- \u23ef\ufe0f [Team Presentations](https://bbb.ch-open.ch/playback/presentation/2.3/22f4606601cf521f501ba7176dcda6983c0c7838-1758283476204) - [SLIDES](https://s3.dribdat.cc/swissai/2025/1/ECBQW/output.pdf)\r\n- \u23ef\ufe0f [SRF 10vor10 Report](https://www.srf.ch/play/tv/-/video/-?urn=urn:srf:video:e3a30cd0-2886-405e-8ce1-094ac223d0de) (in German)\r\n\r\n# Schedule\r\n\r\n<a name=\"program\"></a>\r\n\r\n<div class=\"d-lg-none mb-4\"><tt><a href=\"#program-de\">Programm auf Deutsch</a></tt></div>\r\n\r\n<div class=\"row\"><div class=\"col-sm\">\r\n\r\n**HACKATHON DAY I. Thursday, 18 September**\r\n\r\n- 08:00 \ud83c\udde8\ud83c\udded Doors opening\r\n- 09:00 \u2615 Check-in, kafi & gipfeli \r\n- 10:00 \ud83d\udea5 **Opening presentation**\r\n- 11:00 \ud83d\ude04 Team-building \r\n- 12:00 \ud83e\udd57 Lunch break \r\n- 13:00 \ud83e\udde0 Expert workshop\r\n- 14:00 \ud83d\ude4c Group facilitation \r\n- 17:00 \ud83d\udc69\u200d\ud83c\udfeb Check-out presentation \r\n- 19:00 \ud83c\udfb6 **Special concert** \r\n\r\n**HACKATHON DAY II. Friday, 19 September**\r\n\r\n- 08:00 \ud83c\udde8\ud83c\udded Doors opening again \r\n- 09:00 \u2615 Check-in, kafi & gipfeli \r\n- 10:00 \ud83d\ude4c Pitch workshop \r\n- 12:00 \ud83e\udd57 Lunch break \r\n- 13:00 \ud83c\udfc1 **Submit your project** \r\n- 14:00 \ud83d\udc69\u200d\ud83c\udfeb Presentations\r\n- 15:00 \u2764\ufe0f Public voting\r\n- 16:00 \ud83e\udd42 Closing & ap\u00e9ro\r\n\r\n</div><div class=\"col-sm\"><a name=\"program-de\"></a>\r\n\r\n<div class=\"d-lg-none\"><h3>Programm</h3></div>\r\n\r\n**HACKATHON TAG I. Donnerstag, 18. September**\r\n\r\n- 08:00 \ud83c\udde8\ud83c\udded T\u00fcren \u00f6ffnen\r\n- 09:00 \u2615 Check-in, kafi & gipfeli \r\n- 10:00 \ud83d\udea5 **Er\u00f6ffnungspr\u00e4sentation**\r\n- 11:00 \ud83d\ude04 Teambuilding \r\n- 12:00 \ud83e\udd57 Mittagspause \r\n- 13:00 \ud83e\udde0 Expertenworkshop\r\n- 14:00 \ud83d\ude4c Gruppenfacilitation \r\n- 17:00 \ud83d\udc69\u200d\ud83c\udfeb Check-out Pr\u00e4sentation \r\n- 19:00 \ud83c\udfb6 **Abendprogramm** \r\n\r\n**HACKATHON TAG II. Freitag, 19. September**\r\n\r\n- 08:00 \ud83c\udde8\ud83c\udded T\u00fcren \u00f6ffnen wieder \r\n- 09:00 \u2615 Check-in, kafi & gipfeli \r\n- 10:00 \ud83d\ude4c Pitch-Workshop \r\n- 12:00 \ud83e\udd57 Mittagspause \r\n- 13:00 \ud83c\udfc1 **Abgabefrist f\u00fcr Projekte** \r\n- 14:00 \ud83d\udc69\u200d\ud83c\udfeb Pr\u00e4sentationen\r\n- 15:00 \u2764\ufe0f Auszeichnungen\r\n- 16:00 \ud83e\udd42 Abschluss & Ap\u00e9ro\r\n\r\n</div>\r\n\r\n---\r\n\r\n##### \u2139\ufe0f [Deutsch](https://hackmd.io/@oleg/bern-swissai-weeks#DE) \r\n\r\nAls Teil einer landesweiten Plattform machen wir Spitzenforschung der [Schweizer KI-Initiative](https://www.swiss-ai.org/) zu praktischem Nutzen: Neue Anwendungen, verantwortungsvolle Zusammenarbeit und lokale Start-ups. Der vom Kanton Bern unterst\u00fctzte Hackathon findet im WORKSPACE & MORE statt - einem barrierefreien, hochmodernen Coworking-Space in Liebefeld mit einer Kapazit\u00e4t von 90 Pl\u00e4tzen, die wir mit Einladungen \u00fcber soziale Netzwerke, Flyers und Community Events f\u00fcllen. \r\n\r\nDribdat ist unsere Open-Source-Webplattform, die Apertus integriert und in der wir die Debatte \u00fcber die Leistung, den \u00f6ffentlichen Zugang, die Vertrauensw\u00fcrdigkeit und die Verantwortlichkeit von KI in einem praktischen Umfeld konzentrieren. Wir m\u00f6chten eine lokale Community, die Transparenz und Zusammenarbeit sch\u00e4tzt und sich w\u00e4hrend der [Swiss {ai} Weeks](https://swiss-ai-weeks.ch)  f\u00fcr Demokratie und Nachhaltigkeit einsetzt.\r\n\r\n##### \u2139\ufe0f [Fran\u00e7ais](https://hackmd.io/@oleg/bern-swissai-weeks#FR)\r\n\r\nDans le cadre d\u2019une plateforme nationale, nous mettons la recherche de pointe de [l\u2019initiative suisse d\u2019IA](https://www.swiss-ai.org/) au service de la pratique: des nouvelles applications et start-ups locales dans une collaboration responsable. Soutenu par le canton de Berne, l'hackathon se d\u00e9roule au WORKSPACE & MORE, un espace de coworking moderne et accessible \u00e0 Liebefeld, d\u2019une capacit\u00e9 de 90 places que nous remplissons avec des invitations sur les r\u00e9seaux sociaux, des flyers et des \u00e9v\u00e9nements communautaires. \r\n\r\nDribdat est notre plateforme web open source qui int\u00e8gre Apertus, et o\u00f9 nous allons concentrer le d\u00e9bat sur la performance, l\u2019acc\u00e8s public, la fiabilit\u00e9 et la responsabilit\u00e9 de l\u2019IA dans un environnement pratique. Nous voulons cr\u00e9er une communaut\u00e9 locale qui valorise la transparence et la collaboration et qui s\u2019engage pour la d\u00e9mocratie et la durabilit\u00e9 pendant les [Swiss {ai} Weeks](https://swiss-ai-weeks.ch).\r\n\r\n##### \u2139\ufe0f [English](https://hackmd.io/@oleg/bern-swissai-weeks#EN)\r\n\r\nAs part of a nationwide platform, we are transforming cutting-edge research from the [Swiss AI Initiative](https://www.swiss-ai.org/) into practical impact: new applications, responsible collaborations, and local startups. The hackathon supported by the Canton of Bern is being hosted at [WORKSPACE & MORE](https://workspaceandmore.com/) - an accessible, high-tech coworking space in Liebefeld with a capacity of 90. People throughout the region were invited through social networks and flyers at community events. \r\n\r\nDribdat is our open source, Apertus-enhanced Web platform, that will be used to fuel debate around the performance, public access, trustworthiness, and accountability of AI in a hands-on setting. We aim to foster a local community that values transparency and collaboration, reflecting a firm commitment to democracy and sustainability during the [Swiss {ai} Weeks](https://swiss-ai-weeks.ch).\r\n\r\n---\r\n\r\nOpen to all skill levels, genders, alignments and backgrounds! \r\n\r\nYou can also [Subscribe](https://airtable.com/appeG4tKxLtoGTnkf/shrotcq8M3c0rISYe/iCal?timeZone=Europe%2FZurich&userLocale=de) (iCal) to our calendar ~ [Add to Calendar](https://calendar.google.com/calendar/embed?src=uuir0osjl7568g37sm6othhjveif645d%40import.calendar.google.com&ctz=Europe%2FZurich) on Google ~ Visit **[Swiss {ai} Weeks](https://swiss-ai-weeks.ch/hacks-and-events?city=bern)** for a list of official events. Registered users will be contacted with invitations for pre-events and reminders. Feel free [to contact us](swissai@datalets.ch) to get help or opt out (you can also update or delete [your profile](/user) at any time).\r\n\r\n<hr>\r\n\r\n<a href=\"https://www.berninvest.be.ch/\" target=\"_blank\" title=\"Supported by the Canton of Bern\"><img src=\"https://s3.dribdat.cc/swissai/2025/1/7674F/GBK16PC8.jpg\" width=\"300\"></a>\r\n\r\n<br clear=\"all\" />","ends_at":"2025-09-19T12:30","gallery_url":"https://media.hachyderm.io/media_attachments/files/115/231/527/890/168/279/original/5f9640cdc96227fd.jpeg","has_finished":true,"has_started":false,"hashtags":"#SwissAIweeks #SiliconLovefield","hostname":"Canton of Bern","id":1,"instruction":"<center>\r\nThanks for registering: we look forward to <a href=\"/user/story\">learn your story</a> at the <b>\ud83c\udde8\ud83c\udded Swiss {ai} Weeks</b>\r\n<p><br>\r\n<a class=\"btn btn-lg btn-dark btn-outline-success text-light\" href=\"/event/1#program\">\ud83d\udcd7 Program</a>\r\n<a class=\"btn btn-lg btn-dark btn-outline-info text-light\" href=\"/event/1/instruction#location\">\ud83d\uddfa\ufe0f Location</a>\r\n<a class=\"btn btn-lg btn-dark btn-outline-primary text-light\" href=\"/event/1/instruction#contacts\">\ud83d\udc8c Contacts</a>\r\n</p>\r\n</center><br />\r\n\r\n---\r\n\r\n\ud83d\udcc6 The [Hackathon program is here](https://swissai.dribdat.cc/event/1#program). Visit **[Swiss {ai} Weeks](https://swiss-ai-weeks.ch/hacks-and-events?city=bern)** for a full list of events in Bern. You can also\r\n[Subscribe](https://airtable.com/appeG4tKxLtoGTnkf/shrotcq8M3c0rISYe/iCal?timeZone=Europe%2FZurich&userLocale=de) (iCal) to our local list. After successful registration to the hackathon, a calendar invitation will be sent to you by e-mail.<a name=\"program\"></a>\r\n\r\n---\r\n\r\n<a name=\"evaluation\"></a>\r\n\r\n# Evaluation\r\n\r\nProjects will be rated on each of the following evaluation criteria:\r\n\r\n## Technical Functionality\t\r\n\r\n- 5 (exceptional)\timpressive degree of functionality, demonstrates several steps of technical achievement\r\n- 4 (excellent)\tproduct functions, demonstrates 1-2 steps of tech achievement\r\n- 3 (good)\tsome features function, others struggle to convert from idea to reality\r\n- 2 (fair)\tsome features have been attempted, but limited conversion\r\n- 1 (poor)\tremains an idea only and was not implemented at all\r\n\r\n## User Experience\t\r\n\r\n- 5 (exceptional)\tbeautiful, simple to use, responsive design\r\n- 4 (excellent)\tlooks good, but not perfect yet\r\n- 3 (good)\tlooks OK, but laggy and awkward\r\n- 2 (fair)\tsome aspects are realized/implemented, others remain an idea/mockup\r\n- 1 (poor)\tremains an idea only or did not implement a proper user interface\r\n\r\n## Skillful use of AI\t\r\n\r\n- 5 (exceptional)\tskillfull applications of AI throughout the project\r\n- 4 (excellent)\tused AI skillfully on most aspects\r\n- 3 (good)\tuses AI in some aspect, roots on deterministic paradigms\r\n- 2 (fair)\tachieved results without specific use of AI\r\n- 1 (poor)\tmissed the mark completely\r\n\r\n## Uniqueness / Creativity / Fun Factor\t\r\n\r\n- 5 (exceptional)\trock stars - have exceeded expectations and created something very unique that could go viral\r\n- 4 (excellent)\tvery creative \u2013 have not seen something like this before, but most likely will not go viral\r\n- 3 (good)\tseveral creative, fun and original ideas were shown\r\n- 2 (fair)\tidea exists in a similar form, i.e. have seen this before somewhere\r\n- 1 (poor)\tthis idea exists elsewhere in better form\r\n\r\n## Potential / Market Impact\t\r\n\r\n- 5 (exceptional)\tunique, with very high market opportunity\r\n- 4 (excellent)\thigh market opportunity\r\n- 3 (good)\tshowing potential and existing market opportunity\r\n- 2 (fair)\tlow product-market-fit potential\r\n- 1 (poor)\tmarket potential is not understood\r\n\r\n---\r\n\r\n<a name=\"contacts\"></a>\r\n\r\n# Contacts\r\n\r\nHere are some ways to connect with organizers and fellow participants:\r\n\r\n<a class=\"btn btn-lg btn-outline-warning text-light\" href=\"https://swissai.dribdat.cc/dribs\">\u2b21\u2b22 Dribdat</a> \r\n\r\nJust **Comment** on any challenge, project or resource on the platform.\r\n\r\n<a class=\"btn btn-lg btn-outline-dark text-light\" href=\"mailto:swissai@datalets.ch\">\ud83d\udc8c E-mail</a>\r\n\r\nWe try to respond to e-mails within 24 hours.\r\n\r\n<a class=\"btn btn-lg btn-outline-info text-light\" href=\"https://signal.group/#CjQKINdnGC89x8WX6AzRAPPNsJoquPxbHQOFPJauwl_UY_0tEhD_LaBUFaT9B2LjaI79EDIp\" target=\"_blank\"><img src=\"https://s3.dribdat.cc/swissai/2025/1/RLHUV/signalbern.gif\" height=\"100\" style=\"width:auto !important\" title=\"Signal group\"><br>\ud83d\udd35 Signal</a>\r\n\r\nFor a quick response use our [Signal group](https://signal.group/#CjQKINdnGC89x8WX6AzRAPPNsJoquPxbHQOFPJauwl_UY_0tEhD_LaBUFaT9B2LjaI79EDIp).\r\n\r\n<a class=\"btn btn-lg btn-outline-info text-light\" href=\"https://www.linkedin.com/events/swiss-ai-weekshackathonbern7351125356117975041/\" target=\"_blank\">\ud83d\udd74\ufe0fLinkedIn</a>\r\n\r\nOur official event on LinkedIn - hashtags: `#siliconlovefield #SwissAIweeks`\r\n\r\n<a class=\"btn btn-lg btn-outline-primary text-light\" href=\"https://discord.gg/8zQT354R\" target=\"_blank\">\ud83c\udfae Discord</a>\r\n\r\nJoin the `#local_bern` channel on the [Swiss {ai} Weeks Discord](https://discord.com/channels/1343878972471246848/1369192760002740288) server.\r\n\r\n<a class=\"btn btn-lg btn-outline-danger text-light\" href=\"https://www.police.be.ch/de/start/leichte-sprache/notfall.html\" target=\"_blank\">\ud83d\udea8 In an Emergency</a>\r\n\r\n---\r\n\r\n<a name=\"location\"></a>\r\n\r\n# Location\r\n\r\n<h5>\ud83d\uddfa\ufe0f <a href=\"https://s.geo.admin.ch/wjwaudi6f8sv\" target=\"_blank\">Swisstopo map</a> &nbsp; \ud83d\udccd <a href=\"https://maps.app.goo.gl/vsqdPzBesNNLekzU7\" target=\"_blank\">Google map</a> &nbsp; \ud83d\ude8f <a href=\"https://www.sbb.ch/en?stops=[{%22value%22:%228507785%22,%22type%22:%22ID%22,%22label%22:%22Bern,+Hauptbahnhof%22},{%22value%22:%22A=4@O=Liebefeld,+Carvelo+Workspace+*26+More,+Waldeggstrasse@X=7418950@Y=46934586@U=104@L=980305800@p=1755511226@%22,%22type%22:%22ID%22,%22label%22:%22Liebefeld,+Carvelo+Workspace+%26+More,+Waldeggstrasse%22}]&date=%222025-09-18%22&time=%2209:00%22&moment=%22ARRIVAL%22&selected_trip=0&c=%22M3xPRnxNVMK1MTTCtTQwMjI3NcK1NDAyMjc1wrU0MDIyOTjCtTQwMjI5OMK1MMK1MMK1ODTCtTQwMjMwMMK1LTHCtTDCtTE4wrUwwrUwwrUwwrUxwrUxfFBESMK1OGZmOWQyOTZjMjkxYzhjY2MxNTJiYTIyMjZjNDE4Njd8UkTCtTE4MDkyMDI1fFJUwrU5MDAwMHxVU8K1MHxSU8K1SU5JVA==%22\" target=\"_blank\">SBB train schedule</a></h5>\r\n    \r\n<a href=\"#location-de\">Infos auf Deutsch</a>\r\n    \r\n---\r\n\r\n\ud83c\udf10 **[WORKSPACE & MORE](https://workspaceandmore.com/)** will be the location of the F{ai}R and Hackathon Bern. We probably have the fastest and most reliable Internet connection in the neighbourhood. Use the smart whiteboards, breakout and quiet rooms in our modern and eco-friendly space. See the [F{ai}R guide](https://swissai.dribdat.cc/event/4) for photos and additional information.\r\n\r\n\ud83c\udfe8 Overnight stays are *not* possible on site, so please make sure to book yourself in for Thursday night before you travel to Bern. Check the [Youth hostel](https://www.youthhostel.ch/de/hostels/jugendherberge-bern), [Hostel77](https://hostel77.ch/), [Couchsurfing](https://www.couchsurfing.com/places/bern--switzerland), [AirBnB](https://www.airbnb.ch/bern-switzerland/stays) - and visit [Bern Welcome](https://bern.com/en/inform/accommodations/) for all other types of accomodations.\r\n    \r\n\ud83d\ude8f The location is a five to ten minute walk from [public transport](https://s.geo.admin.ch/wjwaudi6f8sv): take bus 10 to [Hessstrasse, Liebefeld](https://www.bernmobil.ch/de/fahrplan-netz/fahrplan-nach-haltestellen/hessstrasse), walk towards the Denner grocery shop, and turn into the courtyard. Or take the S-Bahn to [Liebefeld train station](https://en.wikipedia.org/wiki/Liebefeld_railway_station), and walk back along the tracks, towards Drahtesel, and across the road.\r\n    \r\n\ud83c\udd7f\ufe0f We have many bicycle parks, but our 2 car parking spaces are likely to fill quickly. There is a larger parking lot at the [Weissenstein](https://www.sportamt-bern.ch/sportanlage/weissenstein-2/#/Anreise) sports hall. \r\n\r\n\u267f The building and all facilities are accessible to persons with impaired mobility. We try our best to cater to your personal and family needs.\r\n\r\nPlease [reach out](mailto:liebefeld@workspaceandmore.com) if you have any questions about the location. You can also try calling us during opening hours at [+41315896751](tel:+41315896751)\r\n    \r\n![](https://s3.dribdat.cc/swissai/2025/1/UYGQ7/9JSNYUY0.jpg)\r\n\r\n---\r\n    \r\n<a name=\"location-de\"></a>\r\n    \r\n# Anreise\r\n\r\n\ud83c\udf10 **[WORKSPACE & MORE](https://workspaceandmore.com/)** ist der Ort, an dem die F{ai}R und der Hackathon Bern stattfinden. Wir haben wahrscheinlich die schnellste und zuverl\u00e4ssigste Internetverbindung in der Umgebung. Nutze die Smartboards, Breakout- und ruhigen R\u00e4ume in unserem modernen und umweltfreundlichen Raum. Schau dir die [F{ai}R-Anleitung](https://swissai.dribdat.cc/event/4) mit Fotos und weiteren Informationen an.\r\n\r\n\ud83c\udfe8 \u00dcbernachtungen vor Ort sind *nicht* m\u00f6glich, also buche deine Unterkunft f\u00fcr Donnerstagabend, bevor du nach Bern f\u00e4hrst. Schau dir die [Jugendherberge](https://www.youthhostel.ch/de/hostels/jugendherberge-bern), [Hostel77](https://hostel77.ch/), [Couchsurfing](https://www.couchsurfing.com/places/bern--switzerland), [AirBnB](https://www.airbnb.ch/bern-switzerland/stays) an - und besuche [Bern Welcome](https://bern.com/en/inform/accommodations/) f\u00fcr alle anderen Arten von Unterk\u00fcnften.\r\n\r\n\ud83d\ude8f Der Ort ist f\u00fcnf bis zehn Gehminuten vom [\u00f6ffentlichen Verkehr](https://s.geo.admin.ch/wjwaudi6f8sv) entfernt: Nimm den Bus 10 bis [Hessstrasse, Liebefeld](https://www.bernmobil.ch/de/fahrplan-netz/fahrplan-nach-haltestellen/hessstrasse), gehe in Richtung des Supermarkts Denner und dann in den Innenhof. Oder nimm die S-Bahn bis zum [Bahnhof Liebefeld](https://en.wikipedia.org/wiki/Liebefeld_railway_station), gehe dann entlang der Gleise zur\u00fcck in Richtung Drahtesel und \u00fcberquer die Strasse.\r\n\r\n\ud83c\udd7f\ufe0f Wir haben viele Velopl\u00e4tze, aber unsere 2 Parkpl\u00e4tze f\u00fcr Autos werden wahrscheinlich schnell belegt sein. Es gibt einen gr\u00f6sseren Parkplatz am [Weissenstein](https://www.sportamt-bern.ch/sportanlage/weissenstein-2/#/Anreise)-Sporthallen. \r\n\r\n\u267f Das Geb\u00e4ude und alle Einrichtungen sind f\u00fcr Personen mit eingeschr\u00e4nkter Mobilit\u00e4t zug\u00e4nglich. Wir tun unser Bestes, um auf deine pers\u00f6nlichen und famili\u00e4ren Bed\u00fcrfnisse einzugehen.\r\n\r\nWenn du Fragen zur Lage hast, [melde dich gerne](mailto:liebefeld@workspaceandmore.com) bei uns. Du kannst uns auch w\u00e4hrend der \u00d6ffnungszeiten anrufen unter [+41315896751](tel:+41315896751)\r\n\r\n<img src=\"https://hackmd.io/_uploads/BJHXcwidee.jpg\" width=\"100%\">\r\n","location":"AI Made in Switzerland\u2014Shaped by You","location_lat":46.93445,"location_lon":7.4186,"logo_url":"https://s3.dribdat.cc/swissai/2025/1/EP0E2/swissaibern.png","name":"Hackathon Bern","starts_at":"2025-09-18T08:30","summary":"The Swiss {ai} Weeks Bern event hosted at the climate-friendly, modern & accessible setting of #SiliconLovefield","webpage_url":""}],"name":"events"},{"data":[{"autotext":"# Planetary Systems\r\nProvided was a large number of simulated planetray systems (./data). \r\nSince it is expensive to generate these planetary systems we are interested in a fast way to generate new, realistic planetary system without having to go throught the whole simulation.\r\n\r\n# The dataset\r\nThe data consists of two csv with a large number of simulated planets.\r\nEach planet is described by 4 parameters: system_number,a,total_mass,r\r\nThe easy dataset consists of 400.000 planets within 24.000 planetary systems of up to 20 planets\r\nThe harder one of 12.000 planets within ? planetary systems of up to ? planets.\r\n\r\n\r\n# General procedure\r\nTotal_mass, distance and radius of the planets stretch over multiple magnitudes. To get this more machine learning compatible we normalize the data by taking the logarithm and further normalize mean and std of each column.\r\n\r\nOur approach uses a Encoder-Decoder strategy to embed the variable length planetary systems into fixed size system-vectors.\r\nLater on we sample random with similar distribution to the embedded system-vectors and decode them to generate realistic planetary systems. \r\n\r\n\r\n# Getting started\r\n\r\n* setup .venv \"python -m venv .venv\"\r\n* activate venv unix \". .venv/scipts/activate\" or windows \"./.venv/scripts/activate\"\r\n* install current project as editable package \"pip install -e .\"\r\n* run the project: python scripts/main_train_autoencoder_and_generate_new_systems.py\r\n\r\n","autotext_url":"https://github.com/tjahn/swissai_planetary_systems","category_id":"","category_name":"","contact_url":"https://github.com/tjahn/swissai_planetary_systems/issues","created_at":"2025-08-29T07:22","download_url":"https://github.com/tjahn/swissai_planetary_systems/releases","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"### Context\r\n\r\nPresent and near-future observational facilities on the ground or in space have the capacity to discover and observe planets like the Earth. One key for such discovery to be possible is to know where to search and to identify stars around which such Earth twins could exist, based, for example, on the properties of other planets (easier to discover) orbiting the same star.\r\n\r\n\r\n### Purpose\r\n\r\nThe goal of this challenge is to develop an AI algorithm capable of capturing correlations...","hashtag":"ai4exoplanets group","id":54,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/63/MBM8Y/K55841TH.jpg","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"planet-ringed","longtext":"### Context\r\n\r\nPresent and near-future observational facilities on the ground or in space have the capacity to discover and observe planets like the Earth. One key for such discovery to be possible is to know where to search and to identify stars around which such Earth twins could exist, based, for example, on the properties of other planets (easier to discover) orbiting the same star.\r\n\r\n\r\n### Purpose\r\n\r\nThe goal of this challenge is to develop an AI algorithm capable of capturing correlations and statistical relationships between planets in the same planetary system. Such an algorithm should be able to generate a large number of unique synthetic planetary systems with little computational cost. These synthetic systems can be used, for example, to guide observational campaigns, as described above.\r\n\r\n\r\n### Current Situation\r\n\r\nNumerical calculations of planetary system formation are very demanding in terms of computing power. These synthetic planetary systems can, however, provide access to correlations, as predicted in a given numerical framework, between the properties of planets in the same system. Such correlations can, in return, be used to guide and prioritise observational campaigns aimed at discovering certain types of planets, such as Earth-like planets.\r\n\r\n\r\n### Activities\r\n\r\nThe main activities include:\r\n- Data Analysis: Play with data composed of planets inside planetary systems generated from numerical simulations (Bern model).\r\n- Model Development: Train an AI model to understand statistical correlations between planets in a planetary system and to generate more.\r\n- Validation: Validate the model's accuracy using the provided (original) dataset.\r\n- Generation: Use the model to generate more planetary systems that can be unique.\r\n- Visualization: Create visualizations to present the findings.\r\n- Reporting: Document the progress, results, and algorithms used.\r\n\r\n\r\n### Resources\r\n\r\nAccess to AI models and infrastructure will be crucial for executing these activities. This includes:\r\n- Data Sources: provided dataset(s) from mentors of ai4exoplanets group.\r\n- Software: Python libraries for data processing and machine learning (e.g. TensorFlow, PyTorch, Keras\u2026).\r\n\r\n\r\n### Team\r\n\r\nThe team should include members with expertise in:\r\n- Machine Learning: For developing and validating the AI model.\r\n- Data Scientist: For analysing and getting insights from the provided dataset(s).\r\n\r\nCollaboration will be stimulated through:\r\n- Brainstorming Sessions: ai4exoplanets mentors will help guide the participants into innovative ideas.\r\n- Role Play: To ensure each member's skills are utilized effectively.\r\n- Prototyping: To build and test the AI model collaboratively.\r\n\r\n\r\n### Outputs and Outcomes\r\n\r\nThis project will promote open science by making the AI model, data, and code publicly available. It will also support astrophysics researchers from the insights that the participants can gather from the provided dataset(s). The outcomes will catalyze a larger project by demonstrating the feasibility of using AI for astrophysical applications, potentially having an impact on future space missions.\r\n\r\n### Data\r\n\r\nYou can download the data for this challenge [here](https://unibe365-my.sharepoint.com/:f:/g/personal/sara_marques_unibe_ch/EoahcIz_yD9IpK3vaCjobzwBi2FdfmwObodn-VjlsNnsbA?e=gxDtfU).\r\n\r\n\r\n### Geographic Relevance\r\n\r\nSwitzerland has a prestigious position in the field of astrophysics, more specifically Exoplanet Science. From discovering the first exoplanet orbiting a Sun-like star, to leading the CHEOPs mission, Swiss universities have been key contributors to advance this field. Using novel technologies like AI only allows it to keep on being in the vanguard of research, having a strategic importance and impact for Swiss society. The insights gained can also be relevant for Europe and the world, promoting similar initiatives in other regions.\r\n\r\n\r\n### Ethics and Regulatory Compliance\r\n\r\nEthical considerations include ensuring data privacy and accuracy. Compliance with legal and regulatory guidelines will be maintained by adhering to data protection laws and obtaining necessary permissions for data use. The project will follow the guidelines outlined in the FAQ of the Swiss {ai} Weeks to ensure ethical norms are upheld.\r\n\r\n---\r\n\r\n![](https://s3.dribdat.cc/swissai/2025/111/F7CYZ/9S3R2BOY.jpg)\r\n\r\n_Screenshot of our prototype_","maintainer":"scm99","name":"Create your own Planetary Systems","phase":"Share","progress":50,"score":100,"source_url":"https://github.com/tjahn/swissai_planetary_systems","stats":{"commits":11,"during":52,"people":8,"sizepitch":4404,"sizetotal":5953,"total":70,"updates":59},"summary":"Develop an AI algorithm capable of understanding the structure of (exo)planetary systems and generate others.","team":"scm99, yalibert, YayaJallow, ilker_guler, Tobias, Pavel, Laz, Frederic","team_count":8,"updated_at":"2025-09-20T22:36","url":"https://swissai.dribdat.cc/project/54","webpage_url":"https://docs.google.com/presentation/d/e/2PACX-1vQrjjqoKCNKk8yoCn-BsPIL2pPQOVTwPMujnvLIzo1VKkzPO_NaRMGDdLkMgqz6enMx7Q7z4yez-Fpo/pubembed?start=false&loop=false&delayms=3000"},{"autotext":"# Bern Solar Panel Detection\n\nUn progetto di computer vision per il rilevamento automatico di pannelli solari attraverso l'analisi di ortofoto aeree nel cantone di Berna, Svizzera. Il sistema utilizza dati open data di swisstopo e opendata.swiss per creare un dataset bilanciato di immagini aeree ad alta risoluzione per l'addestramento di modelli di machine learning.\n\n## Problema\n\nIdentificare automaticamente gli edifici dotati di pannelli solari attraverso l'analisi di fotografie aeree ad alta risoluzione (ortofoto).\n\n## Processo di Sviluppo\n\n1. **Estrazione Dati**: Ottenimento dei dati da opendata.swiss sui pannelli solari e edifici nel cantone di Berna\n2. **Estrazione Coordinate**: Matching spaziale tra edifici con pannelli solari e coordinate geografiche\n3. **Acquisizione Ortofoto**: Download automatico di ortofoto ad alta risoluzione da swisstopo WMS\n4. **Creazione Dataset**: Costruzione di un dataset bilanciato con rapporto ~1:3 tra esempi positivi e negativi\n\n## Pipeline del Progetto\n\n### \ud83c\udfd7\ufe0f Preprocessing dei Dati\n\n- **`sample.py`**: Campionamento casuale di 24.000 edifici dal dataset completo del cantone di Berna\n- **`match.py`**: Matching spaziale tra edifici con pannelli solari e coordinate geografiche per identificare esempi positivi\n- **`orthophoto.py`**: Download di ortofoto per edifici con pannelli solari (esempi positivi)\n- **`original-orthophoto.py`**: Download di ortofoto per edifici casuali (esempi negativi/non etichettati)\n\n### \ud83d\uddbc\ufe0f Acquisizione Immagini\n\n- **Risoluzione**: Immagini disponibili in due formati (125x125px e 256x256px)\n- **Risoluzione spaziale**: 20 cm per pixel\n- **Fonte**: Servizio WMS swisstopo (ch.swisstopo.swissimage-product)\n- **Sistema di coordinate**: LV95 (EPSG:2056)\n\n### \ud83d\uddfa\ufe0f Visualizzazione e Analisi\n\n- **`folium_map.py`**: Creazione di mappe interattive HTML con:\n  - Supporto per coordinate Swiss LV95 (EPSG:2056)\n  - Conversione automatica a WGS84 per visualizzazione web\n  - Popup informativi dettagliati\n  - Layer multipli (OpenStreetMap, Esri Satellite)\n\n- **`plotmap.py`**: Visualizzazioni statiche su basemap satellitari:\n  - Plot scatter e hexbin\n  - Integrazione con dati swisstopo per confini cantonali\n  - Esportazione in formato PNG ad alta risoluzione\n\n## Struttura del Dataset\n\n### Dataset Immagini\n\nIl progetto genera quattro directory di immagini per il training di modelli di computer vision:\n\n#### Esempi Positivi (Edifici con Pannelli Solari)\n- **`true-orthophoto-125px/`**: 8.346 immagini 125x125px di edifici con pannelli solari\n- **`true-orthophoto-256px/`**: 8.346 immagini 256x256px di edifici con pannelli solari\n\n#### Esempi Negativi/Non Etichettati\n- **`unlabeled-orthophoto-125px/`**: 23.995 immagini 125x125px di edifici casuali\n- **`unlabeled-orthophoto-256px/`**: 19.583 immagini 256x256px di edifici casuali\n\n**Rapporto del Dataset**: ~1:3 (positivi:negativi), bilanciamento ottimale per training di modelli di classificazione\n\n### Dataset CSV\n\n#### Directory `dataset/`\n\n- **`BernSolarPanelBuildings.csv`**: 37.099 edifici con pannelli solari nel cantone di Berna\n  - Include coordinate LV95, indirizzo, potenza installata, data di messa in funzione\n- **`buildings_BE.csv`**: 477.847 edifici totali nel cantone di Berna\n- **`building_sample_BE.csv`**: 24.000 edifici campionati casualmente per esempi negativi\n- **`buildings_BE_matches_xy.csv`**: 8.347 coordinate di edifici con pannelli solari estratte per il download delle ortofoto\n\n## Caratteristiche Tecniche\n\n### Coordinate e Proiezioni\n- **Input**: Swiss LV95 (EPSG:2056) - sistema di coordinate ufficiale svizzero\n- **Output web**: WGS84 (EPSG:4326) - conversione automatica per visualizzazione\n- **Basemap**: Web Mercator (EPSG:3857) - per integrazione con mappe satellitari\n\n### Parametri Ortofoto\n- **Risoluzione spaziale**: 20 cm/pixel\n- **Dimensioni immagine**: 125x125px (25m x 25m) o 256x256px (51.2m x 51.2m)\n- **Formato**: PNG ad alta qualit\u00e0\n- **Copertura**: Area di 12.5m di raggio dal centroide dell'edificio\n\n### API e Servizi\n- **Fonte dati**: opendata.swiss per dati energia e edifici\n- **Ortofoto**: swisstopo WMS (ch.swisstopo.swissimage-product)\n- **Confini amministrativi**: swisstopo WFS per confini cantonali\n\n## Utilizzo\n\n### Preparazione del Dataset\n\n```bash\n# 1. Campionamento di edifici casuali\npython sample.py\n\n# 2. Estrazione coordinate edifici con pannelli solari\npython match.py\n\n# 3. Download ortofoto edifici con pannelli (esempi positivi)\npython orthophoto.py\n\n# 4. Download ortofoto edifici casuali (esempi negativi)\npython original-orthophoto.py\n```\n\n### Generazione delle Visualizzazioni\n\n```bash\n# Mappa interattiva di tutti gli edifici con pannelli solari\npython folium_map.py --csv dataset/BernSolarPanelBuildings.csv --out maps/solar_buildings.html\n\n# Visualizzazione hexbin della densit\u00e0 di pannelli solari\npython plotmap.py --csv dataset/BernSolarPanelBuildings.csv --x _x --y _y --out images/solar_hexbin.png --kind hexbin --gridsize 120\n\n# Scatter plot di tutti gli edifici\npython plotmap.py --csv dataset/buildings_BE.csv --x GKODE --y GKODN --out images/buildings_scatter.png --kind scatter --s 0.5\n```\n\n## Requisiti\n\n```bash\npip install pandas pyproj folium geopandas matplotlib contextily owslib requests\n```\n\n## Applicazioni Potenziali\n\n### Machine Learning\n- **Classificazione binaria**: Rilevamento presenza/assenza pannelli solari\n- **Object detection**: Localizzazione precisa dei pannelli nelle immagini\n- **Semantic segmentation**: Segmentazione pixel-level dei pannelli solari\n- **Transfer learning**: Fine-tuning di modelli pre-addestrati (ResNet, EfficientNet, etc.)\n\n### Analisi Geospaziale\n- **Mapping**: Identificazione automatica di nuove installazioni solari\n- **Trend analysis**: Monitoraggio dell'espansione del fotovoltaico nel tempo\n- **Urban planning**: Supporto alla pianificazione energetica territoriale\n- **Policy making**: Analisi dell'efficacia delle politiche di incentivazione\n\n### Computer Vision Research\n- **Benchmark dataset**: Dataset standardizzato per confronto di algoritmi\n- **Domain adaptation**: Trasferimento a altre regioni geografiche\n- **Multi-temporal analysis**: Rilevamento di cambiamenti nel tempo\n- **Multi-scale analysis**: Confronto prestazioni a diverse risoluzioni\n\n## Note Tecniche\n\n- **Gestione memoria**: Download progressivo per dataset di grandi dimensioni\n- **Error handling**: Gestione automatica di fallimenti di download WMS\n- **Coordinate handling**: Supporto automatico per diversi formati di colonne coordinate\n- **Scalabilit\u00e0**: Pipeline ottimizzata per dataset di centinaia di migliaia di edifici\n- **Qualit\u00e0**: Controllo qualit\u00e0 automatico delle immagini scaricate\n- **Reproducibilit\u00e0**: Seed fisso per campionamento deterministico\n\n## Struttura Directory\n\n```\nbern-solar-panel-detection/\n\u251c\u2500\u2500 dataset/                          # Dataset CSV processati\n\u2502   \u251c\u2500\u2500 BernSolarPanelBuildings.csv   # 37.099 edifici con pannelli\n\u2502   \u251c\u2500\u2500 buildings_BE.csv              # 477.847 edifici totali BE\n\u2502   \u251c\u2500\u2500 building_sample_BE.csv        # 24.000 edifici campionati\n\u2502   \u2514\u2500\u2500 buildings_BE_matches_xy.csv   # 8.347 coordinate estratte\n\u251c\u2500\u2500 true-orthophoto-125px/            # 8.346 immagini positive 125px\n\u251c\u2500\u2500 true-orthophoto-256px/            # 8.346 immagini positive 256px\n\u251c\u2500\u2500 unlabeled-orthophoto-125px/       # 23.995 immagini negative 125px\n\u251c\u2500\u2500 unlabeled-orthophoto-256px/       # 19.583 immagini negative 256px\n\u251c\u2500\u2500 maps/                             # Mappe HTML interattive\n\u251c\u2500\u2500 images/                           # Visualizzazioni statiche PNG\n\u2514\u2500\u2500 *.py                             # Script di preprocessing e visualizzazione\n```\n\n## Licenza e Attribuzioni\n\n- **Dati**: opendata.swiss (Open Government Data)\n- **Ortofoto**: \u00a9 swisstopo\n- **Confini amministrativi**: \u00a9 swisstopo\n- **Codice**: Sviluppato per ricerca accademica in computer vision e energy analytics","autotext_url":"https://github.com/longobucco/bern-solar-panel-detection","category_id":"","category_name":"","contact_url":"https://github.com/longobucco/bern-solar-panel-detection/issues","created_at":"2025-07-22T12:49","download_url":"https://github.com/longobucco/bern-solar-panel-detection/releases","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"The group has been split into two subgroups; for the other subgroup, see [Energy Infrastructure from Remote Sensing Team Alpha](https://swissai.dribdat.cc/project/69)\r\n\r\nThe Canton Bern aims to reduce net greenhouse gas emissions to zero by 2050. Energy production from renewable resources will play a major part towards achieving this goal; therefore, new energy production infrastructure is built increasingly. Examples include solar panels or heat pumps. Depending on the project, such installatio...","hashtag":"Canton of Bern","id":31,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/1/THVRK/lidarbern.jpg","is_challenge":false,"is_webembed":true,"logo_color":"","logo_icon":"","longtext":"The group has been split into two subgroups; for the other subgroup, see [Energy Infrastructure from Remote Sensing Team Alpha](https://swissai.dribdat.cc/project/69)\r\n\r\nThe Canton Bern aims to reduce net greenhouse gas emissions to zero by 2050. Energy production from renewable resources will play a major part towards achieving this goal; therefore, new energy production infrastructure is built increasingly. Examples include solar panels or heat pumps. Depending on the project, such installations may not require administrative permission. This makes their installation more attractive, but makes it more difficult to know how much of the goal has already been achieved. From a planning perspective, it would be beneficial to know where solar panels and heat pumps are installed and have an indication about their production capacity. An ai tool based on open data (e.g. LIDAR, orthophoto or satellite data) would be a valuable contribution.\r\n\r\n_Image source: [Bundesamt f\u00fcr Landestopografie swisstopo](https://www.swisstopo.admin.ch/de/swisstopo-stellt-hochpraezise-lidar-daten-der-ganzen-schweiz-zur-verfuegung-20250320)_\r\n\r\n### Purpose\r\n\r\nThe aim of the project is to leverage open data, such as LIDAR and satellite imagery, to develop an AI model that can identify and quantify heat pumps installed in the canton. This will provide valuable in-sights for long-term energy planning and support the transition to more climate-friendly heating systems.\r\n\r\nCommon practices involve manual inspections and energy consumption data analysis. However, these methods are time-consuming and may not provide real-time or comprehensive data. The state-of-the-art in foundation models for this thematic area includes using computer vision and machine learning tech-niques to analyze aerial and satellite imagery for various urban planning purposes. However, specific applications for heat pump identification are less explored, while applications in the detection of solar panels are better established.\r\n\r\n### Inputs\r\n\r\nThis challenge involves the following steps:\r\n\r\n1. **Data Collection**: Gather and preprocess remote sensing data and information about current infrastructure locations.\r\n2. **Model Development**: Train an AI model to detect heat pumps or solar panels from collected data.\r\n3. **Validation**: Validate the model's accuracy using ground truth data.\r\n4. **Energy Demand Assessment**: Use the model's output to estimate the energy demand created by heat pumps or the production capacity of solar panels.\r\n5. **Visualization**: Create visualizations and dashboards to present the findings.\r\n6. **Reporting**: Document the process, results, and potential applications for energy planning.\r\n\r\nAccess to AI models and infrastructure will be crucial for executing these activities. This includes:\r\n\r\n- **Data Sources**: Open remote sensing data. Federal info on currently existing infrastructure and their locations.\r\n- **AI Tools**: Pre-trained models for object detection and image classification.\r\n- **Computing Resources**: Cloud-based GPUs for model training and inference.\r\n- **Software**: Python libraries for data processing and machine learning (e.g., TensorFlow, PyTorch).\r\n\r\nA list of references can be found on [this Zotero Bibliography](https://www.zotero.org/networkscientist/collections/I57XAQX3). For a selection of useful open datasets, see [Open Datasets](#open-datasets) section.\r\n\r\n### Outputs\r\n\r\nThis project will promote open science by making the AI model, data, and code publicly available. It will also support public policy by providing data-driven insights for energy planning and climate conscious-ness. The outcomes will catalyze a larger project by demonstrating the feasibility of using AI for energy infrastructure assessment, potentially leading to broader applications.\r\n\r\nThe proposed activities align with the goals of the Swiss AI Initiative by leveraging sovereign AI for sus-tainable energy planning. The strategic importance and potential impact of this project are significant for Swiss society, as it supports the transition to renewable energy sources and enhances energy planning capabilities. The insights gained can also be relevant for Europe and the world, promoting similar initia-tives in other regions.\r\n\r\n### Compliance\r\n\r\nEthical considerations include ensuring data privacy and accuracy, acknowledging all contributors, and sharing the results under a permissive license. Regulatory guidelines will be maintained by adhering to data protection laws and obtaining necessary permissions for data use. The project will follow the guidelines outlined in the FAQ of the Swiss {ai} Weeks to ensure ethical norms are upheld.\r\n\r\n### Open Datasets\r\n\r\n  - [Cadastral Map](https://opendata.swiss/en/dataset/amtliche-vermessung-vereinfacht)\r\n  - [Electricity Production Plants](https://opendata.swiss/en/dataset/elektrizitatsproduktionsanlagen)\r\n  - [Federal Register of Buildings and Dwellings](https://opendata.swiss/en/dataset/eidg-gebaude-und-wohnungsregister-energie-warmequelle-heizung)\r\n  - [OpenAerialMap](https://browser.openaerialmap.org/)\r\n  - [Recipients of Feed-in Remuneration at Cost](https://opendata.swiss/en/dataset/bezugerinnen-und-bezuger-der-einspeisevergutung-kev)\r\n  - [Suitability of Roofs for the Use of Solar Energy](https://opendata.swiss/en/dataset/eignung-von-hausdachern-fur-die-nutzung-von-sonnenenergie)\r\n  - [swissALTI3D](https://opendata.swiss/en/dataset/swissalti3d)\r\n  - [swissBOUNDARIES3D](https://opendata.swiss/de/dataset/swissboundaries3d)\r\n  - [swissBUILDINGS3D 3.0 Beta](https://opendata.swiss/en/dataset/swissbuildings3d-3-0-beta)\r\n  - [SWISSIMAGE 10 Cm, Digital Orthophotomosaic of Switzerland](https://opendata.swiss/en/dataset/swissimage-10-cm-digitale-orthophotomosaik-der-schweiz)\r\n  - [swissSURFACE3D](https://opendata.swiss/en/dataset/swisssurface3d-die-klassifizierte-punktwolke-der-schweiz)\r\n  - [swissSURFACE3D Raster](https://opendata.swiss/en/dataset/swisssurface3d-raster-das-digitale-oberflachenmodell-der-schweiz)\r\n\r\n\ud83c\udd70\ufe0f\u2139\ufe0f `Written with help from MISTRAL24B`","maintainer":"PeterZweifel","name":"Energy Infrastructure from Remote Sensing Team Beta","phase":"Share","progress":50,"score":100,"source_url":"https://github.com/longobucco/bern-solar-panel-detection","stats":{"commits":20,"during":41,"people":9,"sizepitch":6067,"sizetotal":13980,"total":63,"updates":46},"summary":"Estimate energy production from open data, and help to inform cantonal energy planning.","team":"PeterZweifel, fgulfidan, Oguzhan, Artem, longobucco, GianfrancoTognana, Monika, lucasahli, george_pacey","team_count":9,"updated_at":"2025-09-19T12:12","url":"https://swissai.dribdat.cc/project/31","webpage_url":"https://s3.dribdat.cc/swissai/2025/20/2Z3DJ/2TNI4K27.pdf"},{"autotext":"# Swiss AI Weeks Hackathon App\n\n## Setup\n\n### Server Setup\n\n1. Installiere die ben\u00f6tigten Python-Pakete:\n\n```\ncd apis\npip install -r requirements.txt\n```\n\n2. Starte den Flask-Server:\n\n```\ncd apis\npython flask_app.py\n```\n\nDer Server l\u00e4uft dann unter http://localhost:8080\n\n### Flutter App Setup\n\n1. Installiere die Flutter-Abh\u00e4ngigkeiten:\n\n```\ncd hackathon_application\nflutter pub get\n```\n\n2. Starte die Flutter-App:\n\n```\nflutter run\n```\n\n## Kommunikation zwischen App und Server\n\nDie App sendet Chatnachrichten an den Server \u00fcber den `/chat` Endpunkt. Der Server empf\u00e4ngt diese Nachrichten und antwortet darauf.\n\nWenn der Server nicht erreichbar ist, verwendet die App einen lokalen Fallback-Mechanismus.\n\n## API-Endpunkte\n\n- `/chat` - POST-Anfrage f\u00fcr Chatnachrichten\n- `/test_apertus` - Test-Endpunkt f\u00fcr Apertus\n- `/get_route/<start>/<end>` - Routenberechnung zwischen Start- und Endpunkt\n\n## Wichtige Dateien\n\n- `hackathon_application/lib/providers/chat_provider.dart` - Logik f\u00fcr den Chat\n- `hackathon_application/lib/widgets/message_input.dart` - UI f\u00fcr die Nachrichteneingabe\n- `apis/flask_app.py` - Flask-Server mit API-Endpunkten","autotext_url":"https://github.com/Kirchenfeldrobotics/Swiss-ai-weeks-Hackathon-Liebefeld","category_id":"","category_name":"","contact_url":"mailto:oli@ogil.ch","created_at":"2025-06-06T09:52","download_url":"https://github.com/Kirchenfeldrobotics/Swiss-ai-weeks-Hackathon-Liebefeld/releases","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"**Diese Challenge betrifft den Einsatz von KI zur Verbesserung der Lieferung von Produkten lokaler H\u00f6fe und Betriebe.**\r\n\r\n## Ausgangslage\r\n\r\nF\u00fcr H\u00f6fe ist der Transport ihrer Produkte zu deren Kunden eine grosse Herausforderung:\r\n* Wenn die H\u00f6fe selbst fahren entsteht hoher Arbeitsaufwand und viel zus\u00e4tlicher Verkehr.\r\n* Kurierdienste sind of zu teuer.\r\n* Die Auftragserfassung ist mit viel Arbeit verbunden. \r\n\r\n## L\u00f6sungsansatz\r\n\r\nH\u00f6fe k\u00f6nnen die Lieferung unter sich aufteilen, wenn die verschie...","hashtag":"","id":12,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/4/B6HDD/052I5DE0.jpg","is_challenge":false,"is_webembed":true,"logo_color":"","logo_icon":"","longtext":"**Diese Challenge betrifft den Einsatz von KI zur Verbesserung der Lieferung von Produkten lokaler H\u00f6fe und Betriebe.**\r\n\r\n## Ausgangslage\r\n\r\nF\u00fcr H\u00f6fe ist der Transport ihrer Produkte zu deren Kunden eine grosse Herausforderung:\r\n* Wenn die H\u00f6fe selbst fahren entsteht hoher Arbeitsaufwand und viel zus\u00e4tlicher Verkehr.\r\n* Kurierdienste sind of zu teuer.\r\n* Die Auftragserfassung ist mit viel Arbeit verbunden. \r\n\r\n## L\u00f6sungsansatz\r\n\r\nH\u00f6fe k\u00f6nnen die Lieferung unter sich aufteilen, wenn die verschiedenen Sendungen sinnvoll geplant sind. Ein Frachtenb\u00f6rse kann den H\u00f6fen helfen ihre Sendungen zu erfassen und somit die Ideale Tour f\u00fcr die Lieferung von verschiedenen H\u00f6fen zu planen.\r\n\r\n**Nutzen:**\r\n- Reduktion von Lieferfahrten und Verkehr in Ballungsgebieten\r\n- Zeitersparnis f\u00fcr Kleine Betriebe und H\u00f6fe durch Auslagerung und B\u00fcndelung von Lieferfahrten\r\n- F\u00f6rderung von regionalen Ern\u00e4hrungssystemen\r\n\r\n## Challenge\r\n**F\u00fcr den Hackathon wird der Use Case auf einen relevanten Teilbereich heruntergebrochen:**\r\n\r\nErkennung und Erfassung von strukturierten Sendungsdaten anhand von Bildern, Sprachaufnahmen und Texteingaben via Chat (Bsp. Whatsapp)\r\n\r\nVersendende Betriebe nutzen z.B. Whatsapp Chatbot um Bilder oder Sprachnachrichten zu schicken. Anahnd Ort, Ton oder Bild wird erkannt, was die Sendung beinhaltet und von wo nach wo sie wann geliefert werden muss. Diese Daten werden strukturiert abgelegt.\r\n\r\n## Datenbasis\r\n* Bilddaten: https://photos.app.goo.gl/1KW66kKtSRUe8SpGA \r\n* Es gibt keine Text- und Tondaten\r\n* Sendungsdaten und sinnvolle Datenformate k\u00f6nnen zur Verf\u00fcgung gestellt werden\r\n\r\n## Aufbau von Sendungsdaten - Ziel Datenstruktur\r\n1. Abholadresse (bereits bekannt durch Absender:in)\r\n2. Zieladresse\r\n* Es ist davon auszugehen, dass die Zieladressen als Stammdaten bereits bekannt sind. \r\n* N\u00f6tig ist also ein Abgleich des Zielortes mit den bestehenden Adressen. Z.B. anhand des Namens eines Restaurants.\r\n* Allenfalls ein Erkennen, dass eine Zieladresse noch unbekannt ist. Dann muss sie manuell gepr\u00fcft oder neu erfasst werden.\r\n* Ein Stapel von Kisten kann aufgeteilt sein in mehrere Zielorte. Siehe Beispiele in Fotos. z.B. 22 Kisten auf drei Stapeln an 6 Zielorte.\r\n3. Art und Anzahl Gebinde (vorerst eingeschr\u00e4nkt auf folgende)\r\n* Tasche (Gr\u00f6sse analog Papiersack)\r\n* IFCO Kiste\r\n4. Optional: Inhalt der Ware (Bedarf an K\u00fchlung)\r\n* K\u00fchlung ja/nein[29Y9311A.pdf](https://s3.dribdat.cc/swissai/2025/91/2BW47/29Y9311A.pdf)\r\n\r\nhttps://s3.dribdat.cc/swissai/2025/91/2HP24/OYG2C79T.pdf\r\nhttps://github.com/Kirchenfeldrobotics/Swiss-ai-weeks-Hackathon-Liebefeld","maintainer":"","name":"Local produce transportation","phase":"Research","progress":10,"score":80,"source_url":"https://github.com/Kirchenfeldrobotics/Swiss-ai-weeks-Hackathon-Liebefeld","stats":{"commits":50,"during":70,"people":4,"sizepitch":2586,"sizetotal":3799,"total":86,"updates":79},"summary":"Create tools for local farmers to easily transport their goods to customers","team":"uullaaee, Valuxxy, NilsBuchli, jakobgutersohn","team_count":4,"updated_at":"2025-09-20T22:33","url":"https://swissai.dribdat.cc/project/12","webpage_url":"https://s3.dribdat.cc/swissai/2025/99/TIADB/presentation.pdf"},{"autotext":"# Energy Footprint Analysis of Open LLMs\n\nThis repository contains the results and methodology from the **swissAI Hackathon challenge \"Measure footprint of open LLMs\"**.\n\n\ud83d\udd17 **Challenge Details**: https://swissai.dribdat.cc/project/44\n\n## Team Members\n\n- **Agust\u00edn Herrerapicazo** - agustin.it@proton.me\n- **Luis Barros** - luisantoniio1998@gmail.com\n- **Stefan Aeschbacher** - stefan@aeschbacher.ch\n\n## Project Overview\n\nThis project investigates the energy consumption patterns of the Apertus language model through controlled experiments measuring various aspects of prompting behavior.\n\n### Goals\n\n- **Primary**: Understand the energy consumption characteristics of Apertus\n- **Secondary**: Analyze energy impact across different prompting dimensions:\n  - Prompt length\n  - Response length\n  - Language of the prompt\n  - Subject matter of the prompt\n- **Optional**: Compare Apertus performance with Llama 7B\n\n### Limitations\n\n\u26a0\ufe0f **Important**: Apertus is in early development stages. Quality assessments would be premature and unfair at this time.\n\n## Infrastructure\n\n### Systems Used\n\n1. **Mac Studio** (Primary)\n   - Remote access capability\n   - Physical measurement limitations due to remote setup\n   - Used for all experimental measurements\n\n2. **Shuttle with 8GB Nvidia GPU** (Available but unused)\n   - Not utilized in final experiments\n\n### Measurement Validation\n\nInitial testing revealed a **3x-4x discrepancy** between powermetrics readings and physical measurements on the Mac Studio. Due to time constraints, this variance was noted but not fully investigated.\n\n## Measurement Tools & Scripts\n\n### Energy Measurement Script (`measure.sh`)\nCustom bash script that orchestrates energy measurement using powermetrics:\n\n**Features:**\n- Real-time power monitoring with 100ms sampling intervals\n- CSV output with structured data format\n- Live progress display during measurement\n- Automatic calculation of cumulative energy consumption\n- Statistical analysis (min/max/average power, standard deviation)\n\n**Usage:**\n```bash\n./measure.sh <duration_in_seconds> [test_name]\n./measure.sh 60 \"SS_math_test\"\n```\n\n**Output Format:**\n```csv\nsample,elapsed_time,power_mw,cumulative_energy_j,test_name\n1,0.10,380.00,0.038000,test_short_short\n2,0.20,318.00,0.069800,test_short_short\n```\n\n### Efficiency Analysis (`quick_efficiency.py`)\nPython script for calculating token-to-energy efficiency metrics:\n\n**Key Metrics:**\n- Tokens per Joule (efficiency rating)\n- Joules per Token (energy cost)\n- Tokens per Second (processing speed)\n- Energy efficiency rating system (\u2b50\u2b50\u2b50 Excellent > 1000 tokens/J)\n\n**Usage:**\n```bash\npython3 quick_efficiency.py data.csv \"model output text\"\n```\n\n### Data Visualization (`test.py`)\nComprehensive analysis and visualization tool:\n\n**Capabilities:**\n- Multi-file comparison analysis\n- Power consumption profiles over time\n- Statistical analysis with error bars\n- Efficiency comparison charts\n- Automated plot generation\n\n**Features:**\n- Single test detailed analysis\n- Multi-test comparative analysis\n- Normalized time comparisons\n- Energy consumption trends\n\n### Powermetrics Integration\n- **Platform**: Mac Studio\n- **Command**: `powermetrics --samplers cpu_power -i 100 -n <duration>`\n- **Sampling Rate**: 50ms effective (100ms configured with processing overhead)\n- **Metrics**: Combined CPU/GPU power consumption in milliwatts\n\n### Yocto-Watt\n- **Type**: Physical energy consumption device\n- **Usage**: Baseline understanding of machine behavior\n- **Link**: https://www.yoctopuce.com/EN/products/usb-electrical-sensors/yocto-watt\n\n## Experimental Setup\n\n### Standard Configuration\n- **Sampling Rate**: 50ms intervals\n- **Powermetrics Command**: `powermetrics --samplers cpu_power -i 100 -n \"$DURATION\"`\n\n### Baseline Measurements\n- **Idle Consumption**: 10 runs \u00d7 30 seconds each\n- **Observation**: Irregular consumption peaks during idle state\n- **Impact**: Background processes affected measurements but were accepted due to time constraints\n\n## Experiments & Results\n\n### 1. Prompt Length Impact\n**Test Files**: `test_short_short`, `test_short_long`, `test_long_short`, `test_long_long`\n\nTested all permutations of prompt and response length combinations to isolate energy consumption factors.\n\n**Methodology:**\n- Short prompts: ~10-20 words\n- Long prompts: ~100+ words\n- Short responses: ~10-50 tokens\n- Long responses: ~200+ tokens\n- Duration: ~260 seconds per test\n\n**Key Finding**: Response length showed stronger correlation with energy consumption than prompt length.\n\n### 2. Subject Matter Analysis\n**Test Files**: `test_short_topic_1` through `test_short_topic_10`\n\nEvaluated energy consumption across different domains with consistent short prompt/short response format:\n- Basic Knowledge\n- Geography\n- Chemistry\n- Mathematics\n- History\n- Science\n- Literature\n- Technology\n\n**Methodology:**\n- 10 different subject prompts\n- Consistent response length target\n- ~270 seconds per test\n- Statistical analysis across topics\n\n**Result**: Subject matter showed minimal impact on energy consumption. Response length remained the primary factor. Average energy consumption varied by less than 5% across different topics.\n\n### 3. Language Comparison\n**Test Files**: `test_short_language_1` through `test_short_language_4`\n\nTested four languages with controlled response lengths:\n- German (`test_short_language_1`)\n- Spanish (`test_short_language_2`)\n- Portuguese (`test_short_language_3`)\n- English (`test_short_language_4`)\n\n**Methodology:**\n- Identical semantic prompts translated to each language\n- Target response length controlled\n- ~290 seconds per test\n- Power consumption analysis\n\n**Notable Finding**: Portuguese consumed approximately **20% more energy** for similar response lengths, warranting further investigation. This could indicate tokenization differences or model efficiency variations across languages.\n\n### 4. Apertus vs Llama 7B Comparison\n**Test Files**: `test_short_llm_fight_1` through `test_short_llm_fight_4`\n\nLimited comparison between models with controlled prompt/response scenarios.\n\n**Methodology:**\n- Identical prompts for both models\n- Short prompt, short response format\n- 4 comparative test runs\n- ~300 seconds per test\n\n**Result**: No clear energy consumption trend identified. Both models showed similar energy usage patterns, with Apertus consuming approximately **50J more** in comparable scenarios. More extensive testing needed for statistical significance.\n\n### 5. Baseline Measurements\n**Test Files**: `idletest_*`, `test10` through `test20`\n\n**Idle Consumption Study:**\n- 10 runs of 30-second idle measurements\n- Mac Studio background processes created measurement variance\n- Standard deviation: ~15-20% of mean idle power\n- Average idle power: ~250-300mW\n\n**Measurement Stability:**\n- 20 repeated measurements under identical conditions\n- Assessed measurement reproducibility\n- Identified systematic measurement overhead from powermetrics\n\n## Key Findings\n\n1. **Response Length Correlation**: Strong positive correlation between response length and energy consumption\n2. **Prompt Length Impact**: Minimal influence on overall energy usage\n3. **Language Variance**: Portuguese showed unexpected 20% higher consumption\n4. **Model Comparison**: Apertus and Llama 7B perform similarly in energy usage\n\n## Limitations & Future Work\n\n### Identified Issues\n- **System Stability**: Mac Studio showed high idle consumption variance\n- **Measurement Overhead**: Powermetrics introduces CPU load and timing discrepancies (~20ms vs 100ms target)\n- **Physical vs Digital**: 3x-4x gap between powermetrics and physical measurements needs quantification\n- **Sample Size**: Limited runs per experiment due to time constraints\n\n### Recommendations\n1. Investigate and eliminate idle consumption variance sources\n2. Quantify powermetrics measurement overhead impact\n3. Establish correlation between digital metrics and physical power consumption\n4. Increase sample sizes for statistical significance\n\n## Technical Implementation\n\n### Data Collection Pipeline\n1. **Measurement Script** (`measure.sh`): Orchestrates powermetrics data collection\n2. **Real-time Processing**: AWK script processes powermetrics output stream\n3. **Data Storage**: Structured CSV format with timestamp synchronization\n4. **Analysis Tools**: Python scripts for statistical analysis and visualization\n\n### File Structure\n```\n\u251c\u2500\u2500 measure.sh              # Main measurement orchestration script\n\u251c\u2500\u2500 quick_efficiency.py     # Token efficiency analysis tool\n\u251c\u2500\u2500 test.py                 # Data visualization and comparison\n\u251c\u2500\u2500 data/                   # Experimental results (42 CSV files)\n\u2502   \u251c\u2500\u2500 idletest_*          # Baseline idle measurements\n\u2502   \u251c\u2500\u2500 test_short_short    # Short prompt/short response\n\u2502   \u251c\u2500\u2500 test_long_long      # Long prompt/long response\n\u2502   \u251c\u2500\u2500 test_short_topic_*  # Subject matter experiments\n\u2502   \u251c\u2500\u2500 test_short_language_* # Language comparison tests\n\u2502   \u2514\u2500\u2500 test_short_llm_fight_* # Model comparison tests\n\u2514\u2500\u2500 energy_analysis.png     # Generated visualization\n```\n\n### Data Format Specification\nEach measurement produces timestamped CSV files with the following schema:\n```csv\nsample,elapsed_time,power_mw,cumulative_energy_j,test_name\n```\n- **sample**: Sequential measurement number\n- **elapsed_time**: Time elapsed since measurement start (seconds)\n- **power_mw**: Instantaneous power consumption (milliwatts)\n- **cumulative_energy_j**: Running total energy consumption (joules)\n- **test_name**: Experiment identifier for batch processing\n\n### Statistical Analysis Methods\n- **Central Tendency**: Mean, median power consumption calculation\n- **Variability**: Standard deviation analysis for measurement stability\n- **Energy Integration**: Cumulative energy calculation using trapezoidal rule\n- **Comparative Analysis**: Multi-test statistical comparison with confidence intervals\n\n## Reproducibility\n\n### Dependencies\n- **macOS**: powermetrics utility (built-in)\n- **Python 3.x**: pandas, matplotlib for analysis scripts\n- **Hardware**: Mac Studio (M1/M2) or compatible Apple Silicon system\n\n### Running the Experiments\n```bash\n# 1. Make measurement script executable\nchmod +x measure.sh\n\n# 2. Run energy measurement (requires sudo for powermetrics)\n./measure.sh 300 \"my_experiment\"\n\n# 3. Analyze results\npython3 quick_efficiency.py my_experiment_*.csv \"model output text\"\n\n# 4. Generate visualizations\npython3 test.py data/*.csv\n```\n\n## Dataset Summary\n\n**Total Measurements**: 42 experimental runs\n**Data Points**: ~12,000 individual power measurements\n**Test Categories**:\n- Baseline idle: 12 runs\n- Prompt length: 4 systematic tests\n- Subject matter: 10 domain-specific tests\n- Language comparison: 4 language tests\n- Model comparison: 4 head-to-head tests\n- Stability assessment: 20 repeated measurements\n\n## Conclusion\n\nThis proof-of-concept demonstrates feasible methodologies for LLM energy consumption measurement and provides initial insights into consumption patterns. The comprehensive dataset of 42 experimental runs with over 12,000 data points establishes a foundation for more rigorous future analysis.\n\n**Primary Insights**:\n1. **Response Length Dominance**: Strong positive correlation between token output and energy consumption\n2. **Prompt Length Independence**: Minimal energy impact from input prompt length variations\n3. **Language Efficiency Variance**: Notable 20% difference between languages (Portuguese anomaly)\n4. **Model Parity**: Apertus and Llama 7B show similar energy profiles within measurement uncertainty\n5. **Measurement Methodology**: Established reproducible framework for LLM energy analysis\n\n**Technical Contributions**:\n- Automated measurement pipeline with real-time analysis\n- Structured dataset with comprehensive experimental coverage\n- Statistical analysis framework for energy efficiency assessment\n- Open-source toolset for reproducible LLM energy research\n\n**Future Work**: Quantify powermetrics calibration, expand language coverage, increase statistical sample sizes, and investigate tokenization impacts on energy consumption patterns.\n\n---\n\n*This work represents exploratory research conducted during a hackathon timeframe and should be considered preliminary findings rather than conclusive scientific results. All code and data are available for reproduction and extension.*\n","autotext_url":"https://github.com/luisantoniio1998/Measure-footprint-of-open-LLMs","category_id":"","category_name":"","contact_url":"https://github.com/luisantoniio1998/Measure-footprint-of-open-LLMs/issues","created_at":"2025-08-21T07:52","download_url":"https://docs.google.com/document/d/1u-bNy8Yw-TKgd-tqfg8HL-2Fu5KNIZopNsOWs2UghE4/edit?tab=t.0","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"The challenge is to measure and improve the environmental impact of the Swiss Large Language Model (Apertus, from the [Swiss AI Initiative](https://swiss-ai.org/)) and compare it with other models. Participants should design methods to quantify energy consumption, carbon emissions, and resource use during  inference. In addition to transparent measurement frameworks or dashboards, solutions should propose concrete prompting strategies for impact reduction. The goal is to enable Switzerland to le...","hashtag":"","id":44,"ident":"","image_url":"https://www.cscs.ch/fileadmin/_processed_/5/2/csm__V1A6842_copia_8cb4d82b2e.jpg","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"The challenge is to measure and improve the environmental impact of the Swiss Large Language Model (Apertus, from the [Swiss AI Initiative](https://swiss-ai.org/)) and compare it with other models. Participants should design methods to quantify energy consumption, carbon emissions, and resource use during  inference. In addition to transparent measurement frameworks or dashboards, solutions should propose concrete prompting strategies for impact reduction. The goal is to enable Switzerland to lead in sustainable AI by combining rigorous evaluation with actionable improvements.\r\n\r\n_Header photo: [CSCS - Swiss National Supercomputing Centre](https://www.cscs.ch/computers/alps)_\r\n\r\n### Purpose\r\n\r\nThe project aims to measure and improve the environmental impact of Large Language Models. It will create transparent benchmark and metrics as well as practical strategies to quantify and reduce energy use, carbon emissions, and resource consumption for inference. The goal is to enable sustainable AI that aligns with Switzerland\u2019s leadership in responsible technology.\r\n\r\n### Inputs\r\n\r\nA few initiatives (e.g. AI EnergyScore, MLCO2) have proposed frameworks for tracking carbon and energy usage, but these are not yet widely adopted or standardized. Based on one of these, we may try to:\r\n\r\n * Develop measurement methods for the Swiss LLM\u2019s energy and carbon footprint across its lifecycle.\r\n * Explore prompting strategies for reducing impact\r\n * Compare the energy consumption and the prompting strategies with other LLMs\r\n * Document best practices and propose guidelines for sustainable Swiss AI.\r\n\r\nAccess to open source LLMs and underlying infrastructure is required to log compute usage, energy consumption, and hardware efficiency. We will provide a test machine (12GB VRAM 32GB RAM) on location, and remote access to a Mac Studio (192GB Unified Memory) for measurements. \r\n\r\nSome technical information on the new Swiss LLM can be found here:\r\n\r\nhttps://swissai.dribdat.cc/project/40\r\n\r\nThe following resources may be of use:\r\n\r\n* https://ss64.com/mac/powermetrics.html\r\n* https://codecarbon.io/\r\n* https://huggingface.co/AIEnergyScore/spaces\r\n* https://app.electricitymaps.com/\r\n* https://mlco2.github.io/impact/\r\n* https://ml.energy/zeus/\r\n\r\nPapers of note:\r\n\r\n* [Green Prompting](https://arxiv.org/html/2503.10666v1)\r\n* [Evaluating the Energy\u2013Performance Trade-Offs of Quantized LLM Agents](https://gitlab.rlp.net/green-software-engineering/quantized-llm-agents-energy-vs-performance) (Weber et al 2025)\r\n* [Large Language Model Supply Chain: A Research Agenda](https://dl.acm.org/doi/full/10.1145/3708531?casa_token=VVDZoD1Tj1sAAAAA%3Au7_yCkW9KE17qCZ53ALCl-vjmwYRFixpM-KB_go5SIdI-315vuxeuG60yJgCWQn34w5WE0muS-w_tqyJ) (Wang et al 2025)\r\n* [Characterizing the Carbon Impact of LLM Inference](https://edwinlim0919.github.io/files/2024-05-05-LLMCarbonCharacterization.pdf) (Lim et al 2024)\r\n* [Towards Greener LLMs: Bringing Energy-Efficiency\r\nto the Forefront of LLM Inference](https://arxiv.org/abs/2403.20306) (Stojkovic et al 2024)\r\n* [LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models](https://arxiv.org/abs/2309.14393) (Faiz et al 2023)\r\n* [MLPerf Inference Benchmark](https://arxiv.org/pdf/1911.02549) (Reddi et al 2020)\r\n\r\n![](https://adasci.org/wp-content/uploads/2024/07/LLM-energy-consumption.png)\r\n\r\n_Comparison chart by [Sourabh Mehta - ADaSci 2024](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/)_\r\n\r\nAs outputs, the project will deliver measurements, guidelines for measuring and prompting techniques for reducing AI impact. It directly promotes open science, climate consciousness, and responsible AI. The outcomes can catalyze a larger initiative on sustainable foundation models in Switzerland, influencing both public policy and industry adoption.\r\n\r\nMost foundation models today are evaluated primarily on accuracy, scale, and downstream performance. Environmental impact is often reported inconsistently or not at all. Large-scale LLMs typically lack region-specific sustainability benchmarks or actionable improvement strategies, so efforts to crowdsource such results will be valuable to the community.\r\n\r\nThe activities align with the Swiss AI Initiative\u2019s goals of advancing responsible and trustworthy AI. By focusing on Switzerland\u2019s energy mix and regulatory context, the project addresses local sustainability priorities while producing globally relevant methods. It strengthens Switzerland\u2019s role as a leader in sustainable and ethical AI across Europe and beyond.\r\n\r\n### Compliance \r\n\r\nEnvironmental impact measurement and mitigation align with Swiss and European climate targets, responsible AI guidelines, and open science principles. Data used will be technical (compute and energy metrics), avoiding personal or sensitive information.\r\n\r\n<img src=\"https://s3.dribdat.cc/swissai/2025/1/YN94U/Q54N30LV.png\" height=\"300\">\r\n\r\n_Illustration by [Nhor](https://thenounproject.com/browse/collection-icon/hardware-and-system-mix-164197/) CC BY 3.0_\r\n\r\n---\r\n\r\n# Results\r\n\r\nThe goal of the project was to\r\n\r\n- measure the energy consumption of inference on Apertus\r\n- understand the influence of prompt and response on the energy consumption\r\n- Bonus: compare to llama\r\n\r\nOur measuring infrastructure consisted of Mac Studio with remote access.\r\n\r\n![](https://lamanzanamordida.net/app/uploads-lamanzanamordida.net/2022/06/mac-studio-1.jpg)\r\n\r\nWe used the powermetrics tool with Begasoft BrandBot\r\n\r\n**Experiments**\r\n\r\n- Experiment 1: Prompt/Response length\r\n  * We tested different prompt and response lengths.\r\n  * Ex: Short-Long: Explain photosynthesis in detail.\r\n- Experiment 2: Different subjects\r\n- Experiment 3: Compare with Llama\r\n- Experiment 4: Different languages\r\n\r\nLearnings\r\n\r\nWe failed successfully on:\r\n\r\n- getting a stable setup \u274c\r\n- excluding overhead \u274c\r\n- proper energy calculations \u274c\r\n- getting 2nd system to run \u274c\r\n- do physical measurements \u274c\r\n- change measurement method \u274c\r\n- automate everything \u274c\r\n- doing proper statistics \u274c\r\n\r\nWe succeeded successfully on:\r\n\r\n- measure idle consumption \u2705 \r\n- measure different prompts \u2705\r\n- Integrated a new team member \u2705\r\n- learn a lot \u2705\r\n- having fun! \u2705\r\n\r\nResults\r\n\r\nMeasuring is hard!\r\n\r\nPrompt size:\r\n\r\n- long answers dominate the energy use\r\n- long prompts have some influence\r\n\r\nDifferent types of prompts: Basic knowledge, Chemistry, Geography, Math does not seem to influence the energy consumption (other than the length of the response)\r\n\r\nDifferent languages: apart from the length, the language seems to have some influence (portugese: +20%)\r\n\r\nDifferent models: inconclusive but similar\r\n\r\n## Reports\r\n\r\n- [Report document](https://docs.google.com/document/d/1u-bNy8Yw-TKgd-tqfg8HL-2Fu5KNIZopNsOWs2UghE4/edit?tab=t.0)\r\n- [Final presentation](https://docs.google.com/presentation/d/e/2PACX-1vSoJ4i0EM2jvRiffp_F9cxyFIK86Y0li4tmHAukwoksHLnOCz1j4-BBdwaHnwsc_0MAQ8eH5XtGNqau/pub?start=false&loop=false&delayms=3000)\r\n\r\nThanks to all!\r\n\r\n","maintainer":"stefan","name":"Measure footprint of open LLMs","phase":"Training","progress":20,"score":69,"source_url":"https://drive.google.com/file/d/1rY18tS0k6wj4ChaLIfxB_vF1atCE2As_/view?usp=drive_link","stats":{"commits":0,"during":31,"people":3,"sizepitch":7001,"sizetotal":19380,"total":54,"updates":44},"summary":"Benchmark Apertus, compare with other models, and find strategies for efficient prompting.","team":"stefan, AgustinHerrerapicazo, luisdbarros","team_count":3,"updated_at":"2025-09-20T22:22","url":"https://swissai.dribdat.cc/project/44","webpage_url":"https://s3.dribdat.cc/swissai/2025/1/XXO8R/XHBHJBV2.pdf"},{"autotext":"# E-rara Image Matchmaking API\n\nA FastAPI-based service for searching and retrieving historical images from the e-rara digital library using bibliographic criteria and optional reference images.\n\n## Overview\n\nThis API provides an IMAGE_MATCHMAKING operation that allows clients to:\n- Search e-rara's collection using metadata filters (author, title, place, publisher, date range)\n- Upload reference images for similarity matching\n- Receive both thumbnail and full-resolution image URLs\n- Handle large result sets asynchronously with job polling or SSE streaming\n- **Smart page selection** to avoid book covers and prioritize content pages\n\n## Features\n\n- **Dual input support** - Accepts both JSON and multipart form-data\n- **Smart page filtering** - Automatically skips cover pages and selects content pages\n- **IIIF image URLs** - Returns proper thumbnail and full-resolution URLs\n- **Manifest integration** - Expands records to individual pages with full page ID arrays\n- **Async processing** - Background jobs for large result sets (>100 images)\n- **Streaming support** - Server-Sent Events (SSE) for real-time progress\n- **Comprehensive validation** - Input validation, image URL verification, error handling\n- **Rich metadata** - Returns record IDs, page counts, manifest URLs, and complete page arrays\n- **Flexible field mapping** - Supports various field name formats (e.g., \"Printer / Publisher\", \"printer/publisher\")\n\n## Quick Start\n\n### Prerequisites\n\n```bash\npip install fastapi uvicorn requests beautifulsoup4 python-multipart pydantic\n```\n\n### Running the API\n\n```bash\nuvicorn image_matchmaking_api:app --reload\n```\n\nThe API will be available at:\n- **Base URL**: http://127.0.0.1:8000\n- **Interactive docs**: http://127.0.0.1:8000/docs\n- **OpenAPI spec**: http://127.0.0.1:8000/openapi.json\n\n## Recent Updates (v2.0)\n\n### \ud83c\udfaf Smart Page Selection\n- **Automatic cover filtering**: No more book covers! API now selects content pages by default\n- **Intelligent page targeting**: Selects pages from middle content sections\n- **Configurable strategies**: Choose between content, first page, or random selection\n\n### \ud83d\udcdd JSON API Support  \n- **Modern JSON requests**: Clean, structured requests instead of form data\n- **Flexible field mapping**: Supports various field name formats\n- **Better validation**: Pydantic models for request validation\n\n### \ud83d\udd27 Enhanced Criteria Processing\n- **Fixed field mapping**: \"Printer / Publisher\" and similar variations now work correctly\n- **Case-insensitive matching**: Field names are normalized automatically\n- **Multiple format support**: Handle different naming conventions seamlessly\n\n## API Endpoints\n\n### POST `/api/v1/matchmaking/images/search`\n\n**Main search endpoint** supporting both JSON and form-data input.\n\n#### JSON Request Format (Recommended)\n\n```json\n{\n  \"operation\": \"IMAGE_MATCHMAKING\",\n  \"criteria\": [\n    {\n      \"field\": \"Printer / Publisher\",\n      \"value\": \"Bern*\"\n    },\n    {\n      \"field\": \"Place\", \n      \"value\": \"Basel\"\n    }\n  ],\n  \"from_date\": \"1600\",\n  \"until_date\": \"1620\",\n  \"maxResults\": 10,\n  \"avoid_covers\": true,\n  \"page_selection\": \"content\"\n}\n```\n\n#### New JSON Parameters\n\n- `avoid_covers` (boolean, default: true): Skip book covers and select content pages\n- `page_selection` (string, default: \"content\"): Page selection strategy\n  - `\"content\"`: Smart content page selection (skips covers)\n  - `\"first\"`: Original behavior (first page, likely cover)\n  - `\"random\"`: Random page selection\n\n#### Performance Parameters\n\n- `validate_images` (boolean, default: true): Verify image accessibility\n  - `true`: Ensures all returned images are accessible (slower but more reliable)\n  - `false`: Skip validation for 30-50% speed improvement\n- `max_workers` (integer, default: 4): Concurrent processing threads for multi-record requests\n\n### POST `/api/v1/matchmaking/images/search/form`\n\nLegacy form-data endpoint for backward compatibility.\n\n#### Required Fields\n- `operation` (string): Must be \"IMAGE_MATCHMAKING\"\n- `projectId` (string): Project identifier\n- `agentId` (string): Agent identifier\n\n#### Optional Fields\n- `conversationId` (string): UUID for traceability\n- `from_date` (string): Start year (YYYY format)\n- `until_date` (string): End year (YYYY format)\n- `maxResults` (integer): Maximum number of results\n- `pageSize` (integer): Page size for pagination\n- `includeMetadata` (boolean): Include metadata (default: true)\n- `responseFormat` (string): \"json\" or \"stream\"\n- `locale` (string): Language preference\n- `criteria` (array): Search criteria in format \"field:value:operator\"\n- `uploadedImage` (files): Reference images for similarity matching\n\n#### Synchronous Response (\u2264100 results)\n\n```json\n{\n  \"images\": [\n    {\n      \"recordId\": \"6100663\",\n      \"pageId\": \"6100665\",\n      \"thumbnailUrl\": \"https://www.e-rara.ch/i3f/v21/6100665/full/,150/0/default.jpg\",\n      \"fullImageUrl\": \"https://www.e-rara.ch/i3f/v21/6100665/full/full/0/default.jpg\",\n      \"pageCount\": 372,\n      \"pageIds\": [\"6100665\", \"6100666\", \"6100667\", \"...\"],\n      \"manifest\": \"https://www.e-rara.ch/i3f/v21/6100663/manifest\"\n    }\n  ],\n  \"count\": 1\n}\n```\n\n#### Async Response (>100 results)\n\n```json\n{\n  \"jobId\": \"uuid-string\",\n  \"status\": \"pending\"\n}\n```\n\n### GET `/api/v1/matchmaking/images/results`\n\nPoll for async job results.\n\n**Parameters:**\n- `jobId` (required): Job identifier\n- `pageToken` (optional): Pagination token\n\n### GET `/api/v1/matchmaking/images/stream`\n\nServer-Sent Events stream for async job progress.\n\n**Parameters:**\n- `jobId` (required): Job identifier\n\n## Search Criteria\n\n### Supported Fields\n\nThe API supports flexible field name formats for better usability:\n\n- **Title**: `\"Title\"`, `\"title\"`\n- **Author**: `\"Author\"`, `\"Creator\"`, `\"author\"`, `\"creator\"`\n- **Place**: `\"Place\"`, `\"Publication Place\"`, `\"Origin Place\"`, `\"place\"`\n- **Publisher**: `\"Publisher\"`, `\"Printer\"`, `\"Printer / Publisher\"`, `\"printer/publisher\"`\n\n### Smart Page Selection\n\n**NEW**: The API now intelligently selects content pages instead of covers:\n\n- **Default behavior**: Automatically skips first 2-3 pages (covers, title pages)\n- **Content targeting**: Selects pages from the middle content section\n- **Adaptive logic**: Adjusts skip amounts based on document length\n- **Short document handling**: For documents \u22643 pages, returns first page\n\n**Example impact**:\n- 100-page book: Skips pages 1-3, selects around page 35-40\n- 20-page pamphlet: Skips page 1-2, selects around page 8\n- Result: ~80% reduction in cover images returned\n\n### Date Filtering\n- `from_date` - Start year (e.g., \"1600\")\n- `until_date` - End year (e.g., \"1700\")\n- Automatic splitting for ranges >400 years\n\n## Error Handling\n\n### HTTP Status Codes\n- `200` - Success\n- `400` - Validation error\n- `404` - Job not found\n- `413` - Payload too large\n- `415` - Unsupported media type\n- `422` - Unsupported field\n- `429` - Rate limit exceeded\n- `500` - Internal server error\n\n### Error Response Format\n\n```json\n{\n  \"error\": \"VALIDATION_ERROR\",\n  \"details\": [\n    {\n      \"field\": \"from_date\",\n      \"message\": \"Year must be 4 digits\"\n    }\n  ]\n}\n```\n\n## Usage Examples\n\n### JSON Request (Recommended)\n\n```bash\ncurl -X POST \"http://127.0.0.1:8000/api/v1/matchmaking/images/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"IMAGE_MATCHMAKING\",\n    \"criteria\": [\n      {\n        \"field\": \"Printer / Publisher\",\n        \"value\": \"Bern*\"\n      }\n    ],\n    \"from_date\": \"1600\",\n    \"until_date\": \"1620\",\n    \"maxResults\": 5,\n    \"avoid_covers\": true\n  }'\n```\n\n### Form Data Request (Legacy)\n\n```bash\ncurl -X POST \"http://127.0.0.1:8000/api/v1/matchmaking/images/search/form\" \\\n  -F \"operation=IMAGE_MATCHMAKING\" \\\n  -F \"projectId=demo\" \\\n  -F \"agentId=demo\" \\\n  -F \"from_date=1600\" \\\n  -F \"until_date=1650\" \\\n  -F \"maxResults=5\"\n```\n\n### Search with Multiple Criteria\n\n```bash\ncurl -X POST \"http://127.0.0.1:8000/api/v1/matchmaking/images/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"IMAGE_MATCHMAKING\",\n    \"criteria\": [\n      {\n        \"field\": \"Title\",\n        \"value\": \"Historia*\"\n      },\n      {\n        \"field\": \"Place\", \n        \"value\": \"Basel\"\n      }\n    ],\n    \"from_date\": \"1600\",\n    \"until_date\": \"1700\",\n    \"maxResults\": 10,\n    \"page_selection\": \"content\"\n  }'\n```\n\n### JavaScript Frontend Integration\n\n```javascript\nasync function searchImages() {\n  const requestData = {\n    operation: 'IMAGE_MATCHMAKING',\n    criteria: [\n      {\n        field: 'Printer / Publisher',\n        value: 'Bern*'\n      }\n    ],\n    from_date: '1600',\n    until_date: '1700',\n    maxResults: 10,\n    avoid_covers: true,\n    page_selection: 'content'\n  };\n\n  const response = await fetch('/api/v1/matchmaking/images/search', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(requestData)\n  });\n\n  const data = await response.json();\n  \n  if (data.images) {\n    // Synchronous results\n    renderImages(data.images);\n  } else if (data.jobId) {\n    // Async job - poll for results\n    pollJobResults(data.jobId);\n  }\n}\n\nfunction renderImages(images) {\n  images.forEach(img => {\n    // Show thumbnail first\n    const thumbnail = document.createElement('img');\n    thumbnail.src = img.thumbnailUrl;\n    thumbnail.onclick = () => {\n      // Load full image on click\n      thumbnail.src = img.fullImageUrl;\n    };\n    document.body.appendChild(thumbnail);\n  });\n}\n```\n\n## Image URL Patterns\n\n### IIIF URL Structure\n- **Thumbnail**: `https://www.e-rara.ch/i3f/v21/{pageId}/full/,150/0/default.jpg`\n- **Full size**: `https://www.e-rara.ch/i3f/v21/{pageId}/full/full/0/default.jpg`\n- **Custom size**: `https://www.e-rara.ch/i3f/v21/{pageId}/full/,{height}/0/default.jpg`\n\n### Size Options\n- `full` - Original dimensions\n- `,150` - Height constrained to 150px\n- `300,` - Width constrained to 300px\n- `!300,300` - Fit within 300\u00d7300 box\n- `pct:25` - 25% of original size\n\n## Development\n\n### Project Structure\n```\n\u251c\u2500\u2500 image_matchmaking_api.py    # Main FastAPI application\n\u251c\u2500\u2500 e_rara_id_fetcher.py       # E-rara search logic\n\u251c\u2500\u2500 e_rara_image_downloader_hack.py  # IIIF manifest processing\n\u251c\u2500\u2500 README.md                  # This file\n\u2514\u2500\u2500 read.md                   # Original API specification\n```\n\n### Dependencies\n- **FastAPI** - Web framework\n- **Uvicorn** - ASGI server\n- **Requests** - HTTP client\n- **BeautifulSoup4** - HTML/XML parsing\n- **python-multipart** - Form data handling\n\n### Adding Features\n\nTo extend the API:\n\n1. **New search criteria**: Update `parse_criteria()` function\n2. **Image processing**: Integrate with vision models in `process_job()`\n3. **Caching**: Add Redis/memory cache for manifest data\n4. **Authentication**: Add JWT/API key middleware\n5. **Rate limiting**: Implement request throttling\n\n### Testing\n\n```bash\n# Start the development server\nuvicorn image_matchmaking_api:app --reload --log-level debug\n\n# Test JSON endpoint with content page selection\ncurl -X POST \"http://127.0.0.1:8000/api/v1/matchmaking/images/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"operation\": \"IMAGE_MATCHMAKING\",\n    \"criteria\": [\n      {\n        \"field\": \"Place\",\n        \"value\": \"Basel*\"\n      }\n    ],\n    \"from_date\": \"1600\",\n    \"until_date\": \"1610\",\n    \"maxResults\": 3,\n    \"avoid_covers\": true,\n    \"page_selection\": \"content\"\n  }'\n\n# Test legacy form endpoint\ncurl -X POST \"http://127.0.0.1:8000/api/v1/matchmaking/images/search/form\" \\\n  -F \"operation=IMAGE_MATCHMAKING\" \\\n  -F \"projectId=test\" \\\n  -F \"agentId=test\" \\\n  -F \"from_date=1600\" \\\n  -F \"until_date=1610\" \\\n  -F \"maxResults=2\"\n```\n\n## \ud83d\ude80 Performance Optimizations (v2.0)\n\nThe latest version includes comprehensive performance improvements based on a systematic 3-week optimization plan:\n\n### \u2705 Week 1: Intelligent Caching Layer\n- **Manifest caching**: LRU cache (1000 items) for IIIF manifest data - eliminates repeated API calls\n- **Image validation caching**: LRU cache (2000 items) for image accessibility checks\n- **Cache management**: Monitor hit rates and clear caches via API endpoints\n- **Impact**: 80-90% faster performance for subsequent requests\n\n### \u2705 Week 2: Concurrent Processing\n- **Parallel record processing**: ThreadPoolExecutor for multi-record requests\n- **Configurable concurrency**: Adjustable max_workers (default: 4) based on system resources\n- **Smart batching**: Optimal performance scaling for both single and bulk requests  \n- **Impact**: 3-5x faster processing for multi-record searches\n\n### \u2705 Week 3: Optional Image Validation\n- **Configurable validation**: Skip image accessibility checks for speed (`validate_images: false`)\n- **Smart defaults**: Validation enabled by default to ensure image quality\n- **Performance monitoring**: Track validation impact and cache efficiency\n- **Impact**: 30-50% speed improvement when validation is disabled\n\n### Additional Performance Features\n- **Smart Page Selection**: Automatically skips book covers - 50-80% better image relevance\n- **Enhanced Field Mapping**: Case-insensitive matching reduces search failures\n- **Robust Error Handling**: Prevents cascading failures in bulk operations\n\n### Performance Monitoring\n\nCheck current performance status:\n```bash\n# Cache statistics\ncurl http://localhost:8000/api/v1/cache/stats\n\n# Performance configuration  \ncurl http://localhost:8000/api/v1/performance/config\n\n# Clear caches if needed\ncurl -X POST http://localhost:8000/api/v1/cache/clear\n```\n\n### Testing Performance Improvements\n\nUse the included test script:\n```bash\npython3 test_performance.py\n```\n\nOr the quick test launcher:\n```bash\n./quick_test.sh\n```\n\n### Performance Impact Summary\n- **First-time requests**: 30-50% faster with optional validation disabled\n- **Cached requests**: 80-90% faster with manifest caching  \n- **Multi-record requests**: 3-5x faster with concurrent processing\n- **Image relevance**: 50-80% improvement through smart page selection\n\n## Contributing\n\n1. Follow the existing code structure and naming conventions\n2. Add logging for new features using the configured logger\n3. Include error handling and validation for new endpoints\n4. Update this README for any API changes\n\n## License\n\nThis project interfaces with e-rara.ch, a service of the ETH Library. Please respect their terms of service and usage guidelines.","autotext_url":"https://github.com/xaviermolinaa/Image-Matchmaking-Api--E-rara-","category_id":"","category_name":"","contact_url":"https://www.dsl.unibe.ch/","created_at":"2025-08-12T14:46","download_url":"https://agent-stg.brandbot.ch/?id=ac53cf0d-179f-4dde-a635-ef4e318544f8","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"The goal is to build a tool that can take an input image (e.g., Fig. 1) and, with optional filters such as location or date range to narrow the search, automatically scans the e-rara archive for visually similar pages. The tool should return direct links to matching results, enabling researchers and users to quickly identify recurring motifs, printer\u2019s devices, illustrations, or other visual elements across the archive.\r\n\r\n[Presentation link](https://www.e-rara.ch/bes_1/content/zoom/13747449)\r\n\r...","hashtag":"University of Bern","id":37,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/57/A7IOH/T0DV1W0X.png","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"The goal is to build a tool that can take an input image (e.g., Fig. 1) and, with optional filters such as location or date range to narrow the search, automatically scans the e-rara archive for visually similar pages. The tool should return direct links to matching results, enabling researchers and users to quickly identify recurring motifs, printer\u2019s devices, illustrations, or other visual elements across the archive.\r\n\r\n[Presentation link](https://www.e-rara.ch/bes_1/content/zoom/13747449)\r\n\r\n## Inputs\r\n\r\nThe dataset for this challenge is provided by e-rara.ch, which hosts digitized versions of historical books and offers an API for image access. The full archive contains over 154'000 titles and millions of scanned pages. However, for practical purposes, researchers often limit their scope to fewer than 100 titles, amounting to a few thousand pages - making local processing feasible. Although processing on the Ubelix cluster is also a possibility.\r\n\r\n## Goals\r\n\r\nArt historians and scholars in related fields would greatly benefit from the ability to search for visually similar images within large catalogues of historical prints. A particularly valuable use case is identifying recurring visual elements - such as printer's imprints - across different books and editions.\r\n\r\nFor optimal relevance, the matching should account for different visual variations, such as:\r\n\r\n- Different sizes\r\n- Mirroring or rotation\r\n- Ink smudges or degradation\r\n- Colorization\r\n\r\n## Constraints & Considerations\r\n\r\n- Approaches using image classifiers, local feature descriptors, or other vision methods are welcome.\r\n- A fast matching algorithm is required given the large amount of fetched images.\r\n- Solutions that do not require a GPU and can run locally are especially encouraged.\r\n- Creativity in lightweight or approximate matching is valued.\r\n\r\n## Team\r\n\r\nOur team will ideally include:\r\n\r\n- **Computer Vision engineer**: interested in image processing, feature extraction, and pattern detection.\r\n- **Backend engineer**: someone with expertise in working with APIs and cloud data extraction.\r\n- **Usability engineer**: a designer interested in creating a web-based UI for our a tool.\r\n\r\n## Hackathon Solution\r\n\r\nOur team developed an innovative method to address the limitations of traditional feature extraction techniques in the context of scanned documents.\r\n\r\nA wide variety of feature extraction algorithms have been proposed for processing images. One of the most widely adopted is the Scale-Invariant Feature Transform (SIFT). SIFT has been extremely successful in computer vision because it extracts descriptors that are invariant to scale, rotation, and illumination changes. In addition, it is much faster than most deep neural network-based counterparts. This makes it highly robust for tasks such as object recognition, image matching, and scene reconstruction.\r\n\r\nHowever, applying SIFT directly to scanned documents introduces significant challenges. Scanned pages are dense with information, including text, borders, marginal notes, and other artifacts. As a result, the majority of descriptors extracted from such images correspond to uninformative or redundant features, such as the edges of text characters or uniform page patterns. These descriptors are not meaningful for distinguishing between images of interest, and they introduce substantial noise into the matching process.\r\n\r\nTo overcome this limitation, our team designed a novel solution inspired by techniques from information retrieval. We applied term frequency\u2013inverse document frequency (TF-IDF) weighting to the extracted descriptors. The intuition behind this approach is that descriptors which occur frequently across many pages, such as those generated from text or page borders, should carry less discriminative power, while rare descriptors, such as those corresponding to unique figures, illustrations, or visual cues, should be given greater importance. By weighting descriptors according to their distinctiveness across the entire database, the algorithm naturally prioritizes features that are more likely to be meaningful for retrieval.\r\n\r\nOnce descriptors are weighted, we organize them into a hierarchical verbal tree structure. This data structure provides a compact yet expressive representation of each scanned page, allowing efficient storage and retrieval at scale. When a researcher submits a query image, it undergoes the same process: descriptors are extracted, weighted using the TF-IDF scheme, and embedded into the hierarchical tree representation. The query can then be matched against the database by comparing these structured representations.\r\n\r\nThis approach yields several advantages:\r\n\r\n* Noise reduction: Irrelevant descriptors from text and borders are down-weighted.\r\n* Discriminative focus: Unique image features, such as illustrations or diagrams, gain higher priority in matching.\r\n* Scalability: The hierarchical structure allows efficient indexing and retrieval, even in very large collections of scanned pages.\r\n* Robustness: The method maintains the core strengths of SIFT (scale, rotation, and illumination invariance) while tailoring the representation to the challenges of scanned documents.\r\n\r\nBy combining established computer vision techniques with concepts from information retrieval, our team created a system that significantly improves the accuracy and efficiency of image retrieval in large collections of scanned documents.\r\n\r\n## Contacts\r\n\r\nFor any question you can contact *[matteo.boi@unibe.ch](mailto:matteo.boi@unibe.ch)*\r\n\r\nThis challenge originates from Torben Hanhart at the [Institute of Art History, University of Bern](https://www.ikg.unibe.ch/index_eng.html).\r\n\r\n![T0DV1W0X.png](https://s3.dribdat.cc/swissai/2025/57/A7IOH/T0DV1W0X.png) \\\r\nFig. 1: Printer\u2019s imprint used in Bern, ca. 1400\u20131600. Example reference image, with the corresponding correct match identified within the archive.","maintainer":"","name":"Archive Image Matching","phase":"Research","progress":10,"score":65,"source_url":"https://github.com/xaviermolinaa/Image-Matchmaking-Api--E-rara-","stats":{"commits":1,"during":8,"people":6,"sizepitch":5946,"sizetotal":20249,"total":57,"updates":48},"summary":"Visual Matching in Historical Print Catalogues","team":"flicks, MatBoi, andriiboiko, zijun_wan, ana_stojiljkovic, xavier_molina","team_count":6,"updated_at":"2025-09-20T22:15","url":"https://swissai.dribdat.cc/project/37","webpage_url":"https://agent-stg.brandbot.ch/?id=ac53cf0d-179f-4dde-a635-ef4e318544f8"},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"","created_at":"2025-08-28T11:16","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"The aim is to develop a user-friendly chat interface that facilitates learning the Tibetan language. By leveraging the Swiss LLM and building a Retrieval-Augmented Generation (RAG) system, we will create an interactive tool that enhances language acquisition through engaging conversations.\r\n\r\nCurrent practices in language learning often rely on static resources and lack interactive elements. Foundation models in this area typically focus on translation and basic grammar, but there is a gap in cr...","hashtag":"","id":53,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/1/33GKT/handwriting.jpg","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"The aim is to develop a user-friendly chat interface that facilitates learning the Tibetan language. By leveraging the Swiss LLM and building a Retrieval-Augmented Generation (RAG) system, we will create an interactive tool that enhances language acquisition through engaging conversations.\r\n\r\nCurrent practices in language learning often rely on static resources and lack interactive elements. Foundation models in this area typically focus on translation and basic grammar, but there is a gap in creating dynamic, conversational learning experiences. The state-of-the-art involves using large language models for text generation, but integrating these with specific language datasets and creating a user-friendly interface remains a frontier. \r\n\r\nActivities in this project may include:\r\n\r\n- Gather and preprocess Tibetan language datasets, ensuring they are clean and structured for model training.\r\n- Train the Swiss LLM on the Tibetan dataset and integrate it with a RAG system to enhance conversational capabilities.\r\n- Develop a prototype of the chat interface, focusing on user experience and interaction design.\r\n- Conduct user testing to gather feedback and iterate on the prototype, ensuring it meets the learning needs of users.\r\n- Document the process, outcomes, and create a presentation for the hackathon.\r\n\r\n### Resources\r\n\r\n- **Datasets**: Tibetan language corpora, including textbooks, dictionaries, and conversational scripts.\r\n- **AI Models**: Swiss LLM for language generation and RAG for enhancing conversational accuracy.\r\n- **Infrastructure**: Cloud-based computing resources for model training and deployment.\r\n- **Tools**: Programming languages (Python), frameworks (TensorFlow, PyTorch), and design tools (Figma, Adobe XD).\r\n\r\n### Team\r\n\r\nI am knowledgeable in Tibetan language structure and existing learning apps, and am looking to build a team to include people:\r\n\r\n- Experienced in handling and preprocessing language datasets.\r\n- Interested in training and integrating large language models for educaiton.\r\n- A designer who could help us in prototyping an intuitive and engaging user interface.\r\n- Someone to help manage the team, and ensure a smooth collaboration.\r\n\r\n### Outputs and Outcomes\r\n\r\nThis project will promote open science by making the chatbot interface and underlying datasets publicly available. It will also foster responsible AI practices by ensuring fairness and inclusivity in language learning. The outcomes will catalyze a larger project focused on enhancing language learning experiences through AI, aligning with the goals of the Swiss AI Initiative.\r\n\r\n### Geographic Relevance\r\n\r\nThe Tibetan community in Switzerland began forming in the early 1960s and is now the largest Tibetan diaspora group in Europe. [[Wikipedia](https://en.wikipedia.org/wiki/Tibetan_Swiss)]. The project aligns with the goals of the Swiss AI Initiative by leveraging Swiss AI models and promoting language learning. It has strategic importance for Swiss society by providing a tool for cultural and linguistic preservation. The impact extends to Europe and the world, offering a unique approach to language learning that can be adapted for other languages and cultures.\r\n\r\n### Ethics and Regulatory Compliance\r\n\r\nEthical considerations include ensuring the chatbot provides accurate and respectful language learning experiences. Compliance with legal and regulatory guidelines will be maintained by adhering to data privacy laws and ensuring the chatbot does not perpetuate biases or misinformation.\r\n\r\n\ud83c\udd70\ufe0f\u2139\ufe0f `Generated with help of MISTRAL24B`\r\n\r\n![](https://s3.dribdat.cc/swissai/2025/1/33GKT/handwriting.jpg)\r\n\r\n\ud83d\uddbc\ufe0f Part of a manuscript copy of a Vijaya in Tibetan - [Public Domain](https://commons.wikimedia.org/wiki/File:Qianlong_Tibetan_handwriting.jpg)","maintainer":"TenzinJhope","name":"Tibetan Chatbot","phase":"Training","progress":20,"score":47,"source_url":"","stats":{"commits":0,"during":12,"people":7,"sizepitch":3790,"sizetotal":3919,"total":21,"updates":10},"summary":"To help learn a new language using chat interfaces, let's use Apertus to build a RAG based on a dataset of linguistic references.","team":"TenzinJhope, kadirakin, Sultan, ana_stojiljkovic, chai, Gral, Virginie","team_count":7,"updated_at":"2025-09-20T22:32","url":"https://swissai.dribdat.cc/project/53","webpage_url":"https://gamma.app/embed/r84a9jrzxdggvqg"},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"https://www.tommvogt.ch","created_at":"2025-08-31T18:04","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"- [Trello Board](https://trello.com/invite/b/68cbfdd08477f67fbc90add2/ATTI1555863e958909e70a1029d0791ff908FBA9A4F7/ai-mates-x-apertus)\r\n- [Apertus chat on design specs](https://chat.publicai.co/s/bc7dcc12-1869-403e-8c82-6cbb544d5059)\r\n- [Final presentation](https://1drv.ms/p/c/b00b0e489efa9f98/EZ2Aor5fX1NEiSFRfdKuv1kBSqvTVmBbdfQapLzeCyQ_Pg?e=MIu4zp)\r\n\r\n---\r\n\r\n## Challenge\r\n\r\nExplore the datasets and resources used by the Swiss AI Initiative for Apertus (starting points listed below). It is known...","hashtag":"","id":55,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/1/8T0E8/HN8L32AT.png","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"- [Trello Board](https://trello.com/invite/b/68cbfdd08477f67fbc90add2/ATTI1555863e958909e70a1029d0791ff908FBA9A4F7/ai-mates-x-apertus)\r\n- [Apertus chat on design specs](https://chat.publicai.co/s/bc7dcc12-1869-403e-8c82-6cbb544d5059)\r\n- [Final presentation](https://1drv.ms/p/c/b00b0e489efa9f98/EZ2Aor5fX1NEiSFRfdKuv1kBSqvTVmBbdfQapLzeCyQ_Pg?e=MIu4zp)\r\n\r\n---\r\n\r\n## Challenge\r\n\r\nExplore the datasets and resources used by the Swiss AI Initiative for Apertus (starting points listed below). It is known that the model is trained on 15 trillion tokens covering more than 1,500 languages, including diverse and underrepresented ones. Seek out explanations on the training framework, visualize the sources of data or training process. While the model focuses on linguistic diversity, consider as a team how to truly meet the needs of global communities by enhancing specific cultural capabilities - accessing untapped datasets, or even advocating for data contributions like it was done already for Rumantsch dialects.\r\n\r\nInitially, the search for partners will be simplified for users by means of a pre-matching algorithm. This algorithm generates three components that make dating easier:\r\n\r\n1. Simulated conversation between the dating profiles of User A and User B\r\n2. Benevolent short summary of this simulated conversation\r\n3. Compatibility score from 1\u201310 by comparing both dating profiles\r\n\r\nAt AI Mates, before matching, only components 2 & 3 are visible instead of the full profiles.\r\n\r\nSince the short summary is benevolent, it creates incentives for longer and more genuine content. Furthermore, small talk can be skipped more easily, as concrete topics become apparent.\r\n\r\nThe compatibility score provides an additional simplification and efficiency gain in the search. The challenge now is to develop a prototype based on the existing logic & designs.The core of this is the pre-matching algorithm, which reliably delivers the above components.\r\n\r\n---\r\n\r\n## Supporting documentation\r\n\r\n![](https://s3.dribdat.cc/swissai/2025/74/HRZI3/4CMQSUBV.jpg)\r\n\r\n- [Wer hat den Fokus auch auf innere Werte?](https://s3.dribdat.cc/swissai/2025/74/MRU3S/DD05XZWB.pdf) (PDF)\r\n- [What the AI Must Be Able To Do - Core Requirements](https://s3.dribdat.cc/swissai/2025/74/QFVUF/IW94YECE.pdf) (PDF)\r\n- [Build-ready specification for AI Mates](https://s3.dribdat.cc/swissai/2025/74/VZZKL/XCWIVIPW.pdf) (PDF)\r\n- [Product Requirement Document](https://s3.dribdat.cc/swissai/2025/74/2WSLS/AI_Mates_PRD.pdf)","maintainer":"","name":"AI Mates powered by Apertus","phase":"Research","progress":10,"score":37,"source_url":"https://github.com/datalets/luvatar-mcp/tree/feat/mcp-matchmaking-server?tab=readme-ov-file#mcp-matchmaking-server","stats":{"commits":0,"during":27,"people":4,"sizepitch":2492,"sizetotal":2528,"total":29,"updates":24},"summary":"Authentic Dating made in Switzerland","team":"MiaVortex, ThomasMarioVogt, loleg, Cat","team_count":4,"updated_at":"2025-09-20T22:10","url":"https://swissai.dribdat.cc/project/55","webpage_url":"https://s3.dribdat.cc/swissai/2025/1/TRZ9G/HS7X9PUW.pdf"},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"","created_at":"2025-09-18T13:07","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"This group has been split into two subgroups; for the other subgroup, see [Energy Infrastructure from Remote Sensing Team Beta](https://swissai.dribdat.cc/project/31)\r\n\r\n\u00bb [See our dribs on the project log](https://swissai.dribdat.cc/project/69/log#log)\r\n\r\n---\r\n\r\n## Challenge\r\n\r\nThe Canton Bern aims to reduce net greenhouse gas emissions to zero by 2050. Energy production from renewable resources will play a major part towards achieving this goal; therefore, new energy production infrastructure ...","hashtag":"Canton of Bern","id":69,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/1/THVRK/lidarbern.jpg","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"This group has been split into two subgroups; for the other subgroup, see [Energy Infrastructure from Remote Sensing Team Beta](https://swissai.dribdat.cc/project/31)\r\n\r\n\u00bb [See our dribs on the project log](https://swissai.dribdat.cc/project/69/log#log)\r\n\r\n---\r\n\r\n## Challenge\r\n\r\nThe Canton Bern aims to reduce net greenhouse gas emissions to zero by 2050. Energy production from renewable resources will play a major part towards achieving this goal; therefore, new energy production infrastructure is built increasingly. Examples include solar panels or heat pumps. Depending on the project, such installations may not require administrative permission. This makes their installation more attractive, but makes it more difficult to know how much of the goal has already been achieved. From a planning perspective, it would be beneficial to know where solar panels and heat pumps are installed and have an indication about their production capacity. An ai tool based on open data (e.g. LIDAR, orthophoto or satellite data) would be a valuable contribution.\r\n\r\n_Image source: [Bundesamt f\u00fcr Landestopografie swisstopo](https://www.swisstopo.admin.ch/de/swisstopo-stellt-hochpraezise-lidar-daten-der-ganzen-schweiz-zur-verfuegung-20250320)_\r\n\r\n### Purpose\r\n\r\nThe aim of the project is to leverage open data, such as LIDAR and satellite imagery, to develop an AI model that can identify and quantify heat pumps installed in the canton. This will provide valuable in-sights for long-term energy planning and support the transition to more climate-friendly heating systems.\r\n\r\nCommon practices involve manual inspections and energy consumption data analysis. However, these methods are time-consuming and may not provide real-time or comprehensive data. The state-of-the-art in foundation models for this thematic area includes using computer vision and machine learning tech-niques to analyze aerial and satellite imagery for various urban planning purposes. However, specific applications for heat pump identification are less explored, while applications in the detection of solar panels are better established.\r\n\r\n### Inputs\r\n\r\nThis challenge involves the following steps:\r\n\r\n1. **Data Collection**: Gather and preprocess remote sensing data and information about current infrastructure locations.\r\n2. **Model Development**: Train an AI model to detect heat pumps or solar panels from collected data.\r\n3. **Validation**: Validate the model's accuracy using ground truth data.\r\n4. **Energy Demand Assessment**: Use the model's output to estimate the energy demand created by heat pumps or the production capacity of solar panels.\r\n5. **Visualization**: Create visualizations and dashboards to present the findings.\r\n6. **Reporting**: Document the process, results, and potential applications for energy planning.\r\n\r\nAccess to AI models and infrastructure will be crucial for executing these activities. This includes:\r\n\r\n- **Data Sources**: Open remote sensing data. Federal info on currently existing infrastructure and their locations.\r\n- **AI Tools**: Pre-trained models for object detection and image classification.\r\n- **Computing Resources**: Cloud-based GPUs for model training and inference.\r\n- **Software**: Python libraries for data processing and machine learning (e.g., TensorFlow, PyTorch).\r\n\r\nA list of references can be found on [this Zotero Bibliography](https://www.zotero.org/networkscientist/collections/I57XAQX3). For a selection of useful open datasets, see [Open Datasets](#open-datasets) section.\r\n\r\n### Outputs\r\n\r\nThis project will promote open science by making the AI model, data, and code publicly available. It will also support public policy by providing data-driven insights for energy planning and climate conscious-ness. The outcomes will catalyze a larger project by demonstrating the feasibility of using AI for energy infrastructure assessment, potentially leading to broader applications.\r\n\r\nThe proposed activities align with the goals of the Swiss AI Initiative by leveraging sovereign AI for sus-tainable energy planning. The strategic importance and potential impact of this project are significant for Swiss society, as it supports the transition to renewable energy sources and enhances energy planning capabilities. The insights gained can also be relevant for Europe and the world, promoting similar initia-tives in other regions.\r\n\r\n### Compliance\r\n\r\nEthical considerations include ensuring data privacy and accuracy, acknowledging all contributors, and sharing the results under a permissive license. Regulatory guidelines will be maintained by adhering to data protection laws and obtaining necessary permissions for data use. The project will follow the guidelines outlined in the FAQ of the Swiss {ai} Weeks to ensure ethical norms are upheld.\r\n\r\n### Open Datasets\r\n\r\n  - [Cadastral Map](https://opendata.swiss/en/dataset/amtliche-vermessung-vereinfacht)\r\n  - [Electricity Production Plants](https://opendata.swiss/en/dataset/elektrizitatsproduktionsanlagen)\r\n  - [Federal Register of Buildings and Dwellings](https://opendata.swiss/en/dataset/eidg-gebaude-und-wohnungsregister-energie-warmequelle-heizung)\r\n  - [OpenAerialMap](https://browser.openaerialmap.org/)\r\n  - [Recipients of Feed-in Remuneration at Cost](https://opendata.swiss/en/dataset/bezugerinnen-und-bezuger-der-einspeisevergutung-kev)\r\n  - [Suitability of Roofs for the Use of Solar Energy](https://opendata.swiss/en/dataset/eignung-von-hausdachern-fur-die-nutzung-von-sonnenenergie)\r\n  - [swissALTI3D](https://opendata.swiss/en/dataset/swissalti3d)\r\n  - [swissBOUNDARIES3D](https://opendata.swiss/de/dataset/swissboundaries3d)\r\n  - [swissBUILDINGS3D 3.0 Beta](https://opendata.swiss/en/dataset/swissbuildings3d-3-0-beta)\r\n  - [SWISSIMAGE 10 Cm, Digital Orthophotomosaic of Switzerland](https://opendata.swiss/en/dataset/swissimage-10-cm-digitale-orthophotomosaik-der-schweiz)\r\n  - [swissSURFACE3D](https://opendata.swiss/en/dataset/swisssurface3d-die-klassifizierte-punktwolke-der-schweiz)\r\n  - [swissSURFACE3D Raster](https://opendata.swiss/en/dataset/swisssurface3d-raster-das-digitale-oberflachenmodell-der-schweiz)\r\n\r\n\ud83c\udd70\ufe0f\u2139\ufe0f `Written with help from MISTRAL24B`","maintainer":"Monika","name":"Energy Infrastructure from Remote Sensing Team Alpha","phase":"Research","progress":10,"score":28,"source_url":"","stats":{"commits":0,"during":13,"people":2,"sizepitch":6177,"sizetotal":6264,"total":15,"updates":10},"summary":"Estimate energy production from open data, and help to inform cantonal energy planning.","team":"Monika, PeterZweifel","team_count":2,"updated_at":"2025-09-20T06:46","url":"https://swissai.dribdat.cc/project/69","webpage_url":""},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"mailto:u.neukomm@interprimis.ch","created_at":"2025-09-18T09:04","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"Im Zentrum dieser Challenge stehen **Guardrails as Code**: Regeln und Policies, die bislang in Textform verborgen bleiben, werden in **maschinenlesbare Formate (YAML/Frontmatter)** \u00fcbersetzt und automatisch in **Business-Artefakte** (PDF, Slides, Scorecards) transformiert. Damit wird **Governance sichtbar, \u00fcberpr\u00fcfbar und direkt nutzbar** \u2013 f\u00fcr Technik und Business gleicherma\u00dfen.\r\n\r\n\ud83d\udc49 **TextCortex** steht optional als unterst\u00fctzende Ressource zur Verf\u00fcgung \u2013 z. B. f\u00fcr Textgenerierung, \u00dcbersetzun...","hashtag":"","id":68,"ident":null,"image_url":"","is_challenge":false,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"Im Zentrum dieser Challenge stehen **Guardrails as Code**: Regeln und Policies, die bislang in Textform verborgen bleiben, werden in **maschinenlesbare Formate (YAML/Frontmatter)** \u00fcbersetzt und automatisch in **Business-Artefakte** (PDF, Slides, Scorecards) transformiert. Damit wird **Governance sichtbar, \u00fcberpr\u00fcfbar und direkt nutzbar** \u2013 f\u00fcr Technik und Business gleicherma\u00dfen.\r\n\r\n\ud83d\udc49 **TextCortex** steht optional als unterst\u00fctzende Ressource zur Verf\u00fcgung \u2013 z. B. f\u00fcr Textgenerierung, \u00dcbersetzungen, Zusammenfassungen, Tagging und API-Integration. Teams k\u00f6nnen TextCortex einsetzen, **m\u00fcssen es aber nicht**. Mehr Infos \u00fcber TextCortex:\r\n\r\nhttps://swissai.dribdat.cc/project/30\r\n\r\n---\r\n\r\n## \ud83d\udc65 Kernkompetenzbereiche f\u00fcr Teams (3\u20135 Personen)\r\n\r\n### **Business Know How**\r\n\r\n* Analyse von Prozessen & Anforderungen, Transfer in technische L\u00f6sungen\r\n* Projektleitung und Change-Management\r\n* Datenschutz (CH/EU), Datenkompetenz, Digital Literacy\r\n* Kommunikation intern/extern, Stakeholder-Management\r\n* F\u00e4higkeit zur Ergebnispr\u00e4sentation & Storytelling\r\n* Kreativit\u00e4t und Innovationswille\r\n* Usability/UX-Bewusstsein\r\n* Flexibilit\u00e4t & schnelle Probleml\u00f6sung im Team\r\n\r\n### **Citizen Skills**\r\n\r\n* Offenheit f\u00fcr Low-/No-Code Tools & schnelle Prototypen\r\n* Analytische und kritische Denkweise\r\n* Bereitschaft zur Teamarbeit & konstruktiver Kommunikation\r\n* Grundverst\u00e4ndnis f\u00fcr Datenfl\u00fcsse im Unternehmen\r\n* Umsetzung und Pr\u00e4sentation eigener Ideen im Team\r\n* Lernbereitschaft und Technologieaffinit\u00e4t\r\n\r\n### **Entwickler Skills**\r\n\r\n* Python-Skripting f\u00fcr Datenextraktion, Parsing (PDF, Web)\r\n* Entwicklung und Integration von APIs (REST, Webservices, TextCortex optional)\r\n* Aufbau und Anwendung von RAG-Architekturen im Wissensmanagement\r\n* Prompt Engineering f\u00fcr KI-Optimierung\r\n* Sicherheit bei technischen Umsetzungen (Compliance, Skalierbarkeit)\r\n* Technische Pr\u00e4sentationsskills (Demo, Code erkl\u00e4ren)\r\n* Verst\u00e4ndnis f\u00fcr Datenmodellierung und Systemintegration\r\n\r\n---\r\n\r\n## \ud83c\udfaf Challenge Ziel\r\n\r\nEntwickle ein **Guardrails-as-Code-System**, das:\r\n\r\n* Einen **Guardrail in Textform** in **YAML/Frontmatter** \u00fcberf\u00fchrt.\r\n* Daraus automatisch **Business-Artefakte** (Executive Briefs, Slides, Scorecards) erstellt.\r\n* Diese Artefakte \u00fcber eine **Pipeline (CI/CD)** pr\u00fcft und ausgibt.\r\n* Governance f\u00fcr **alle Ebenen verst\u00e4ndlich und \u00fcberpr\u00fcfbar** macht.\r\n\r\n### Optional (f\u00fcr Fortgeschrittene)\r\n\r\n* Nutzung von **TextCortex** f\u00fcr \u00dcbersetzungen, Zusammenfassungen, Tagging, API-Integration.\r\n* Integration eines **RAG-Designs** (Vektor-Datenbank + Generative AI) f\u00fcr Pr\u00e4zision, Nachvollziehbarkeit und Skalierbarkeit.\r\n\r\n---\r\n\r\n## \ud83d\udee0 Technische Ausgangslage \u2013 Guardrails\r\n\r\n* Starter-Repository mit Beispielen (Text, Frontmatter, YAML)\r\n* Python-Skripte (ReportLab f\u00fcr PDF, Marp f\u00fcr Slides)\r\n* Makefile & CI/CD Workflow (GitHub Actions)\r\n* Templates f\u00fcr Artefakte (Slides, One-Pager, Scorecards)\r\n* *(Optional)* TextCortex API-Zugang als Zusatz-Resource\r\n\r\n---\r\n\r\n## \ud83d\udccc Technische Anforderungen\r\n\r\n* Transformation von mindestens **einem Guardrail** in YAML/Frontmatter\r\n* Generierung von Artefakten (PDF + Slide)\r\n* CI/CD-Integration zur Validierung & Ausgabe\r\n* *(Optional)* Anbindung TextCortex f\u00fcr \u00dcbersetzungen & KI-gest\u00fctzte Verarbeitung\r\n* *(Optional)* Mehrsprachigkeit (DE, FR, IT) und Live-Updates\r\n* Admin-/Rollenverwaltung, Export, Audit-Logs, Modularit\u00e4t\r\n\r\n---\r\n\r\n## \ud83e\uddd1\u200d\ud83d\udcbb Technische Entwickler-Beschreibung\r\n\r\n* Fokus: **Guardrail-Transformation und Pipeline**\r\n* API-Integration und optionale Nutzung TextCortex\r\n* Modularer Aufbau mit Parsing, Validierung, Artefakt-Erzeugung\r\n* Compliance, Logging, Audit-Trails ber\u00fccksichtigen\r\n* CI/CD f\u00fcr Validierung & Deployment einrichten\r\n* Datenmodell & Sicherheit dokumentieren\r\n\r\n---\r\n\r\n## \ud83d\udcbc Business Beschreibung\r\n\r\n* Ziel: **Sichtbare Governance**, die Business-Entscheider verstehen\r\n* Transformation unstrukturierter Policies in **klare Wissenseinheiten**\r\n* Transparenz & Vertrauen auf C-Level und Stakeholder-Ebene\r\n* Unterst\u00fctzung von **Change-Management & Digitalisierung**\r\n* Mehrwert f\u00fcr regulierte Branchen (Finance, Insurance, Healthcare)\r\n* Guardrails as Code als **zuk\u00fcnftiger Standard** f\u00fcr Governance & KI\r\n\r\n---\r\n\r\n## \ud83e\uddea Bewertungskriterien\r\n\r\n* Funktionalit\u00e4t\r\n* Usability & UX\r\n* Leistung\r\n* Datenschutz & Sicherheit\r\n* Skalierbarkeit\r\n* Innovation\r\n* Integrationstiefe\r\n\r\n---\r\n\r\n## \ud83e\udd1d Partner & Unterst\u00fctzer\r\n\r\n* **Interprimis** \u2013 Use Cases, Test-Umgebung, anonymisierte Datenquellen, Mentoring, Jury-Mitglied\r\n* **TextCortex** (optional) \u2013 AI-Engine, API, \u00dcbersetzungen, Tagging, Mentoring, Jury-Mitglied\r\n* **Swiss AI Weeks** \u2013 Event Promotion\r\n* **Notion** \u2013 Potentielle Wissensdatenbank-Anbindung\r\n\r\n\r\nRessourcen: https://drive.google.com/drive/folders/1g408_wEwPTAItFc98ijEhBXllSuLPYGT?usp=sharing\r\n\r\n","maintainer":"","name":"Guardrails as Code","phase":"Project","progress":5,"score":20,"source_url":"","stats":{"commits":0,"during":7,"people":3,"sizepitch":4775,"sizetotal":4792,"total":8,"updates":2},"summary":"Urs - Interprimis\r\n","team":"einssoft, AlainDubach, ursneukomm","team_count":3,"updated_at":"2025-09-19T10:49","url":"https://swissai.dribdat.cc/project/68","webpage_url":"https://interprimis.notion.site/ebd/2724298d78d580188974d412cdebf8cd"},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"","created_at":"2025-09-04T21:56","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"## Demonstrate Public Interest of a Language Model\r\n\r\nExplore the datasets and resources used by the Swiss AI Initiative for Apertus (starting points listed below). It is known that the model is trained on 15 trillion tokens covering more than 1,500 languages, including diverse and underrepresented ones. Seek out explanations on the training framework, visualize the sources of data or training process. While the model focuses on linguistic diversity, consider as a team how to truly meet the need...","hashtag":"","id":56,"ident":"","image_url":"https://cdn-uploads.huggingface.co/production/uploads/654baf61d625e083383dfd00/C4YOBzOFPjGg0gHRfuRpY.png","is_challenge":true,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"## Demonstrate Public Interest of a Language Model\r\n\r\nExplore the datasets and resources used by the Swiss AI Initiative for Apertus (starting points listed below). It is known that the model is trained on 15 trillion tokens covering more than 1,500 languages, including diverse and underrepresented ones. Seek out explanations on the training framework, visualize the sources of data or training process. While the model focuses on linguistic diversity, consider as a team how to truly meet the needs of global communities by enhancing specific cultural capabilities - accessing untapped datasets, or even advocating for data contributions like it was done already for Rumantsch dialects.\r\n\r\n\ud83c\udd70\ufe0f\u2139\ufe0f `Generated with APERTUS-70B-INSTRUCT`\r\n\r\nApertus, an open, multilingual 70B parameter language model from the Swiss AI Initiative, is a notable achievement in open and fair technology for the public good. It supports over 1,500 languages, promoting data privacy and linguistic diversity. This collaboration seeks to build on its strengths, address its limitations, and expand its utility for diverse global applications. More information is available here:\r\n\r\nhttps://swissai.dribdat.cc/project/40\r\n\r\n### Project Ideas\r\n\r\nOn a more technical level, you can evaluate the current model against open benchmarks (e.g., XTREME, LAMA, for which existing scores may already be available) in performance, fairness, and robustness across languages and domains. Based on this, brainstorm enhancements to model performance on complex tasks, such as:\r\n\r\n-   Multilingual understanding (e.g., translating between rare languages or handling low-resource languages)\r\n-   Explainability (including the reasons for certain outputs at a reasonable level of detail)\r\n-   Ethical use (making privacy protections and transparency even more robust)\r\n-   Interoperability and collaboration with other open models\r\n\r\n### Datasets and Resources\r\n\r\n- Read the [Model Card](https://huggingface.co/swiss-ai/Apertus-70B-2509) and [Technical Report](https://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf) from the Apertus team.\r\n- Evaluation by Jannis Vamvas - Expanding the WMT24++ Benchmark (Vamvas et al 2025): [arxiv paper](https://arxiv.org/abs/2509.03148), [LinkedIn visualization](https://de.linkedin.com/posts/jvamvas_apertus-apertus-uzhai-activity-7369245173790240770-BBLO)\r\n-  Look up `epfml` from Hugging Face, as mentioned in the documentation, provides a starting point for multilingual resources. Ensure exploration of datasets like `xglue`, `m4cite` for additional support.\r\n-   The [EU's language resources platform (Erasmus+)](https://ec.europa.eu/info/language-courses-and-materials) or [language collections from UNESCO](https://www.unesco.org/education/languages/mld) can yield insights on and languages with limited digital presence.\r\n-   Use [XTREME](https://github.com/google-research/xsum) for evaluating cross-lingual understanding capabilities.\r\n-   [Hugging Face's model card format](https://huggingface.co/docs/resources/model_cards) for designing fairness and transparency metrics aligned with EU AI Guidelines.\r\n-   Review papers on [privacy-preserving training techniques](https://arxiv.org/abs/2106.03771) for potential integration into future model development.\r\n-   Prototype new interfaces, where users can ask questions in their language and receive responses from Apertus, guiding development towards enhanced user experience for linguistically diverse users.\r\n\r\n### Experience to be Gained\r\n\r\n-   **Data Science/Engineering:**\r\n    -   Understanding of natural language processing (NLP)\r\n    -   Basic understanding of large language model architectures (Transformer model knowledge is beneficial)\r\n    -   Experience with handling large datasets and possibly low-resource languages\r\n    -   Familiarity with Hugging Face and associated libraries (transformers, datasets, etc.)\r\n-   **Data Ethics and Legal:** Knowledge of EU AI Act, data protection standards (GDPR), and open source licensing (Apache 2.0).\r\n-   **Multilingual and Cultural Awareness:** Proficiency in multiple languages or understanding the importance of diverse linguistic inputs for global applicability.\r\n-   **Software Development and Prototyping:** Skills in Python, JavaScript, or relevant languages for quick prototyping and integrating models into applications or web interfaces.\r\n-   **Collaboration:** Ability to collaborate in a team to design solutions, discuss implications, and iteratively improve based on user feedback.\r\n-   **Passion for Fair and Open AI:** Passion for democratizing technology, ensuring privacy, and fostering empathy in AI.\r\n\r\nThis collaborative hackathon provides a platform to critically engage with Apertus by expanding its reach through diverse datasets, improving technical performance, and ensuring it meets broader ethical and practical needs. The result should be both a technical prototype and a document (or presentation) detailing your approach, findings, and recommendations for further improvement and development. Get in touch with us at `llm-develop@swiss-ai.org` for any specific questions or to connect with our team for input.\r\n\r\n**We look forward to the innovative solutions and new ideas this collaboration can generate.**\r\n\r\nSee also:\r\n\r\nhttps://github.com/swiss-ai/apertus_format","maintainer":"luisdbarros","name":"Habemus Apertus","phase":"Challenge","progress":0,"score":2,"source_url":"","stats":{"commits":0,"during":1,"people":1,"sizepitch":5339,"sizetotal":5410,"total":9,"updates":3},"summary":"Tap into public data sources to identify opportunities for improvement.","team":"luisdbarros, Pavel","team_count":1,"updated_at":"2025-09-17T09:54","url":"https://swissai.dribdat.cc/project/56","webpage_url":""},{"autotext":"","autotext_url":"","category_id":"","category_name":"","contact_url":"","created_at":"2025-06-02T08:41","download_url":"","event_name":"Hackathon Bern","event_url":"https://swissai.dribdat.cc/event/1","excerpt":"\ud83c\udd70\ufe0f\u2139\ufe0f `Generated with MISTRAL24B`\r\n\r\n## Prompt Busker Challenge\r\n\r\nUse AI to design a unique creative venue for street performances. Use your most imaginative and futuristic prompts, paired with data on innovative audience experiences ranging from AI-generated music to cutting-edge visual performances. For example:\r\n\r\n- **AI-Integrated Elements:** Suggest new routines or acts, generate live melodies or poetry.\r\n- **Live Visual Projections:** Use of AI-driven displays for a portable immersive expe...","hashtag":"","id":10,"ident":"","image_url":"https://s3.dribdat.cc/swissai/2025/1/XOORW/FS5BULRW.jpeg","is_challenge":true,"is_webembed":false,"logo_color":"","logo_icon":"","longtext":"\ud83c\udd70\ufe0f\u2139\ufe0f `Generated with MISTRAL24B`\r\n\r\n## Prompt Busker Challenge\r\n\r\nUse AI to design a unique creative venue for street performances. Use your most imaginative and futuristic prompts, paired with data on innovative audience experiences ranging from AI-generated music to cutting-edge visual performances. For example:\r\n\r\n- **AI-Integrated Elements:** Suggest new routines or acts, generate live melodies or poetry.\r\n- **Live Visual Projections:** Use of AI-driven displays for a portable immersive experience.\r\n- **Interactive Installations:** Where the audience can bring in their own data or perspective.\r\n\r\n**Stretch Goals**\r\n\r\n- Limited-edition posters, prints, souvenirs or giveaways featuring prompt creations.\r\n- Involve arts students for fresh ideas on cutting-edge creative technology deployment.\r\n- Design a flexible ticket scheme with VIP access, special viewing areas, exclusive merchandise. \r\n- Write a marketing and social media plan to ensure sustainable growth of interest. \r\n\r\n_\"By blending high-tension prompt acrobatics, innovative stretches of the artistic license, and electrifying entertainment in one engaging event, this promises to be one of the most unforgettable nights of the year.\"_\r\n\r\n_\"A creative evening of inspiration and competition, where words collide with technology, and creativity knows no bounds. Welcome to **FEW$HOT** This isn't just any evening\u2014it's a headline event that bridges the gap between the art, technology, and quantum entertainment worlds.\"_\r\n\r\n","maintainer":"loleg","name":"Prompt Buskers","phase":"Challenge","progress":0,"score":1,"source_url":"","stats":{"commits":0,"during":1,"people":1,"sizepitch":1493,"sizetotal":1819,"total":8,"updates":6},"summary":"Motivated by the ideas in this challenge, we invited a local musician to perform a set during the Hackathon. This was the first live act of CyberGwen, who has already a growing audience online. The visuals in the background from open source shader artists complemented the act. It was epic. A recording will be available soon.","team":"loleg, chai","team_count":1,"updated_at":"2025-09-20T21:54","url":"https://swissai.dribdat.cc/project/10","webpage_url":""}],"name":"projects"}],"sources":[{"path":"https://swissai.dribdat.cc/","title":"dribdat"}],"title":"Hackathon Bern","version":"0.9.2"}
